{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PROC_NAME = 'gpt2-medium'\n",
    "FT_PROC_NAME = 'ft_model'\n",
    "PLUGIN_FT_PROC_NAME = 'plugin_over_ft'\n",
    "PLUGIN_BASE_PROC_NAME = 'plugin_over_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./models/ft_model')\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_PROC_NAME)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "base_model_for_loading = AutoModelForCausalLM.from_pretrained(BASE_PROC_NAME)\n",
    "base_model_for_loading.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = PeftModel.from_pretrained(base_model_for_loading, f\"./models/{FT_PROC_NAME}\")\n",
    "model_ft.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'left'\n",
    "\n",
    "# model_ft = AutoModelForCausalLM.from_pretrained(\"./models/peft_gpt2-medium_e2e\")  # Path to your fine-tuned model\n",
    "\n",
    "# This converts the base_model variable equal to model_ft\n",
    "# model_ft = PeftModel.from_pretrained(base_model_name, \"./models/peft_gpt2-medium_e2e\")\n",
    "\n",
    "# plugin_model = AutoModelForCausalLM.from_pretrained(\"./models/plugin_decoded\") \n",
    "plugin_model_ft = AutoModelForCausalLM.from_pretrained(f\"./models/{PLUGIN_FT_PROC_NAME}\") \n",
    "plugin_model_base = AutoModelForCausalLM.from_pretrained(f\"./models/{PLUGIN_BASE_PROC_NAME}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plugin_model_ft.resize_token_embeddings(len(tokenizer))\n",
    "plugin_model_base.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Compare the state dictionaries\n",
    "# are_same = all((p1 == p2).all() for p1, p2 in zip(model_ft.parameters(), base_model.parameters()))\n",
    "# print(f\"Are the models identical (by weights)? {are_same}\")\n",
    "\n",
    "# # Compare the configurations\n",
    "# are_same_config = model_ft.config == base_model.config\n",
    "# print(f\"Are the models identical (by config)? {are_same_config}\")\n",
    "    \n",
    "# # Check if they are the same object in memory\n",
    "# are_same_object = base_model is model_ft\n",
    "# print(f\"Are the models the same object in memory? {are_same_object}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test dataset (replace with your dataset)\n",
    "dataset = load_dataset(\"e2e_nlg_cleaned\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meaning_to_references = defaultdict(list)\n",
    "for entry in dataset:\n",
    "    meaning_to_references[entry[\"meaning_representation\"]].append(entry[\"human_reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Preprocess the dataset to include the meaning representation (MR) as input and human reference as target\n",
    "# def preprocess_function(examples):\n",
    "#     # Concatenate MR and human reference with a separator\n",
    "#     inputs = [f\"<bos> {mr} <eos>\" for mr in examples[\"meaning_representation\"]]\n",
    "#     targets = [f\"<bos> {ref} <eos>\" for ref in examples[\"human_reference\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
    "#     labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "#     # Replace padding token id's of the labels by -100 so that it's ignored by the loss\n",
    "#     labels[\"input_ids\"] = [\n",
    "#         [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq] \n",
    "#         for labels_seq in labels[\"input_ids\"]\n",
    "#     ]\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the test dataset\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"meaning_representation\"], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DictDataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list of dicts): A list where each element is a dictionary with features as keys.\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the data to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing the features and their corresponding values for the given index.\n",
    "        \"\"\"\n",
    "        example = self.data_list[idx]\n",
    "        \n",
    "        # # Tokenize the 'meaning_representation' on the fly\n",
    "        # tokenized = tokenize_function(example, self.tokenizer)\n",
    "        # inputs = f'<bos> {example[\"meaning_representation\"]} <eos>'\n",
    "        # inputs = f\"Question: Generate a natural language sentence from the following aspects: {example['meaning_representation']}\" + \"\\nAnswer:\"\n",
    "        inputs = example[\"meaning_representation\"]\n",
    "        tokenized = tokenizer(inputs, return_tensors=\"pt\", max_length=64, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        \n",
    "        # Return the tokenized inputs along with any other features (like labels)\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'meaning_representation' :   example['meaning_representation']# Remove batch dimension\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_dataset = DictDataset([{'meaning_representation': mr} for mr in meaning_to_references.keys()], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 3672,    58, 14573, 43537,  4357,  4483,  6030,    58,  1073,  5853,\n",
       "          6128,  4357,  1989,    58, 19205,  7372,    60, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'meaning_representation': 'name[Blue Spice], eatType[coffee shop], area[city centre]'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load BLEU and ROUGE metrics from evaluate library\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "# tokenized_test_data = unique_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "# model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_top_k(probs, top_k):\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "    top_k_probs = torch.nn.functional.softmax(top_k_probs, dim=-1)\n",
    "    next_token = torch.multinomial(top_k_probs, 1)\n",
    "    return top_k_indices.gather(-1, next_token)\n",
    "\n",
    "def sample_top_p(probs, top_p):\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_probs[sorted_indices_to_remove] = 0\n",
    "    next_token = torch.multinomial(sorted_probs, 1)\n",
    "    return sorted_indices.gather(-1, next_token)\n",
    "\n",
    "def apply_temperature(logits, temperature):\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_top_k_top_p(probs, top_k, top_p):\n",
    "    \"\"\"\n",
    "    Sample the next token based on top-k and top-p (nucleus) sampling.\n",
    "    \n",
    "    Args:\n",
    "        probs (torch.Tensor): The probabilities of the next token (shape: [batch_size, vocab_size]).\n",
    "        top_k (int): The number of most probable tokens to consider in top-k sampling.\n",
    "        top_p (float): The cumulative probability cutoff for sampling (nucleus sampling).\n",
    "    \n",
    "    Returns:\n",
    "        next_token (torch.Tensor): The sampled token indices (shape: [batch_size, 1]).\n",
    "    \"\"\"\n",
    "    # Apply top-k sampling: Keep only the top_k most probable tokens\n",
    "    if top_k > 0:\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        probs = torch.zeros_like(probs).scatter_(-1, top_k_indices, top_k_probs)\n",
    "\n",
    "    # Sort probabilities for top-p sampling\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # Mask out tokens that exceed the top-p cumulative probability\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_probs[sorted_indices_to_remove] = 0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)  # Re-normalize\n",
    "\n",
    "    # Sample from the remaining tokens after applying top-k and top-p\n",
    "    next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "\n",
    "    # Return the corresponding token indices from the original vocabulary\n",
    "    return sorted_indices.gather(-1, next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_position_ids(input_ids, attention_mask):\n",
    "#     \"\"\"\n",
    "#     Generate position IDs for left-padded sequences.\n",
    "    \n",
    "#     Args:\n",
    "#         input_ids (torch.Tensor): Tensor of input token IDs (shape: [batch_size, seq_len]).\n",
    "#         attention_mask (torch.Tensor): Tensor of attention mask (shape: [batch_size, seq_len]).\n",
    "        \n",
    "#     Returns:\n",
    "#         torch.Tensor: Position IDs (shape: [batch_size, seq_len]).\n",
    "#     \"\"\"\n",
    "#     # Get the lengths of the non-padded tokens (i.e., count of '1's in the attention mask)\n",
    "#     seq_lengths = attention_mask.sum(dim=-1)\n",
    "\n",
    "#     # Create a tensor with position IDs starting from 0 for each non-padded token\n",
    "#     position_ids = torch.arange(input_ids.size(1), dtype=torch.long).unsqueeze(0).repeat(input_ids.size(0), 1).to(input_ids.device)\n",
    "\n",
    "#     # Adjust position IDs for each sequence to start from 0 after padding\n",
    "#     position_ids = position_ids - (input_ids.size(1) - seq_lengths).unsqueeze(-1)\n",
    "\n",
    "#     # Set position IDs for padding tokens to 0 (optional: you can use another value if needed)\n",
    "#     position_ids = torch.where(attention_mask == 1, position_ids, torch.zeros_like(position_ids))\n",
    "#     return position_ids.long()\n",
    "\n",
    "def get_position_ids(input_ids, attention_mask):\n",
    "    position_ids = torch.arange(input_ids.size(1)).expand_as(input_ids).to(input_ids.device)\n",
    "    position_ids = position_ids * attention_mask\n",
    "    return position_ids.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_repetition_penalty(logits, generated_ids, repetition_penalty):\n",
    "    \"\"\"\n",
    "    Applies repetition penalty to logits tensor based on generated_ids tensor.\n",
    "    \n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits of shape (batch_size, vocab_size).\n",
    "        generated_ids (torch.Tensor): Generated token IDs of shape (batch_size, seq_length).\n",
    "        repetition_penalty (float): The repetition penalty to apply.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Logits after applying the repetition penalty.\n",
    "    \"\"\"\n",
    "    batch_size, vocab_size = logits.shape\n",
    "\n",
    "    # Check for invalid indices in generated_ids\n",
    "    if (generated_ids < 0).any() or (generated_ids >= vocab_size).any():\n",
    "        print(\"Error: `generated_ids` contains values out of range. Please ensure all token IDs are within [0, vocab_size-1].\")\n",
    "        print(f\"generated_ids: {generated_ids}\")\n",
    "        print(f\"Valid range: [0, {vocab_size - 1}]\")\n",
    "        return logits\n",
    "\n",
    "    # Get unique tokens in each batch sequence\n",
    "    unique_tokens = [torch.unique(seq) for seq in generated_ids]\n",
    "\n",
    "    # Find the maximum number of unique tokens in any sequence\n",
    "    max_unique_len = max([len(tokens) for tokens in unique_tokens])\n",
    "\n",
    "    # Pad each unique token list to the max length with a negative index (-1) that is not in the vocabulary\n",
    "    padded_unique_tokens = torch.stack(\n",
    "        [torch.cat([tokens, tokens.new_full((max_unique_len - len(tokens),), -1)]) for tokens in unique_tokens]\n",
    "    )\n",
    "\n",
    "    # Ensure no negative values other than -1 in padded_unique_tokens\n",
    "    if (padded_unique_tokens < -1).any() or (padded_unique_tokens >= vocab_size).any():\n",
    "        print(\"Error: Invalid values in `padded_unique_tokens`. Please ensure all token IDs are within [0, vocab_size-1].\")\n",
    "        print(f\"padded_unique_tokens: {padded_unique_tokens}\")\n",
    "        return logits\n",
    "\n",
    "    # Debugging: Check the padded_unique_tokens shape and values\n",
    "    print(\"Padded Unique Tokens:\\n\", padded_unique_tokens)\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "    # Create a mask of shape (batch_size, vocab_size) where repeated tokens are True\n",
    "    penalty_mask = torch.zeros((batch_size, vocab_size), dtype=torch.bool, device=logits.device)\n",
    "\n",
    "    # Use advanced indexing to set the mask to True for repeated tokens (ignore padding -1)\n",
    "    try:\n",
    "        penalty_mask.scatter_(1, padded_unique_tokens, True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scatter operation: {e}\")\n",
    "        print(\"Check if `padded_unique_tokens` contain valid indices for `penalty_mask`.\")\n",
    "        return logits\n",
    "\n",
    "    # Ensure padding index (-1) is not penalized\n",
    "    penalty_mask[:, -1] = False\n",
    "\n",
    "    # Debugging: Check the penalty_mask\n",
    "    print(\"Penalty Mask Shape:\", penalty_mask.shape)\n",
    "    print(\"Penalty Mask Sample (First Row):\", penalty_mask[0])\n",
    "\n",
    "    # Apply the repetition penalty using vectorized operations\n",
    "    positive_logits_mask = logits > 0\n",
    "    negative_logits_mask = logits < 0\n",
    "\n",
    "    # Check shapes and contents before applying the penalty\n",
    "    print(\"Logits Before Penalty:\\n\", logits)\n",
    "    print(\"Positive Logits Mask:\\n\", positive_logits_mask)\n",
    "    print(\"Negative Logits Mask:\\n\", negative_logits_mask)\n",
    "\n",
    "    # Check the combined mask shape and contents\n",
    "    combined_mask = penalty_mask & positive_logits_mask\n",
    "    print(\"Combined Mask Shape:\", combined_mask.shape)\n",
    "    print(\"Combined Mask (Sample):\", combined_mask[0])\n",
    "\n",
    "    # For positive logits, divide by repetition penalty\n",
    "    try:\n",
    "        logits[combined_mask] /= repetition_penalty\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying penalty to positive logits: {e}\")\n",
    "\n",
    "    # For negative logits, multiply by repetition penalty\n",
    "    try:\n",
    "        logits[penalty_mask & negative_logits_mask] *= repetition_penalty\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying penalty to negative logits: {e}\")\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_generate(model, input_ids, attention_mask, max_length, repetition_penalty, tokenizer, top_k=50, temperature=1.0, top_p = 1, bb_model = None):\n",
    "    \n",
    "    generated_ids = input_ids.clone().to(model.module.device)  # Start with the input prompt\n",
    "    # Create a list to track which sequences have finished\n",
    "    finished_sequences = torch.zeros(input_ids.size(0), dtype=torch.bool).to(model.module.device)\n",
    "        \n",
    "    for step in range(max_length-input_ids.size()[1]):\n",
    "        # Get the model outputs (logits) for the current step\n",
    "        with torch.no_grad():\n",
    "            position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "            outputs = model(input_ids=generated_ids, attention_mask=attention_mask, use_cache=True, position_ids=position_ids)\n",
    "            logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "        \n",
    "#         min_logit = torch.absolute(torch.min(logits)) + 1.0\n",
    "#         logits += min_logit\n",
    "#         # logits = apply_repetition_penalty(logits, generated_ids, repetition_penalty)\n",
    "        \n",
    "#         # # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "#         for i, gen_id in enumerate(generated_ids):\n",
    "#             for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "#                 # if logits[i, token_id] > 0:\n",
    "#                 logits[i, token_id] /= repetition_penalty\n",
    "#                 # else:\n",
    "#                 #     logits[i, token_id] *= repetition_penalty\n",
    "\n",
    "        # Apply temperature\n",
    "        # logits = apply_temperature(logits, temperature)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        if(bb_model):\n",
    "            with torch.no_grad():\n",
    "                position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "                outputs_base = bb_model(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "                logits_base = outputs_base.logits[:, -1, :]  # Get logits of the last token\n",
    "            \n",
    "            # min_logit = torch.absolute(torch.min(logits_base)) + 1.0\n",
    "            # logits_base += min_logit\n",
    "        \n",
    "            # logits_base = apply_repetition_penalty(logits_base, generated_ids, repetition_penalty)\n",
    "            # # # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "            # for i, gen_id in enumerate(generated_ids):\n",
    "            #     for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "            #         # if logits_base[i, token_id] > 0:\n",
    "            #         logits_base[i, token_id] /= repetition_penalty\n",
    "            #         # else:\n",
    "            #         #     logits_base[i, token_id] *= repetition_penalty\n",
    "            #         # logits_base[i, token_id] /= repetition_penalty\n",
    "            probs_base = torch.nn.functional.softmax(logits_base, dim=-1)\n",
    "            # print('middle', torch.max(probs_base, dim=-1))\n",
    "            probs = probs*probs_base\n",
    "            sum_probs = probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "            # Avoid division by zero by adding a small value (epsilon)\n",
    "            sum_probs = torch.clamp(sum_probs, min=1e-9)\n",
    "\n",
    "            # Re-normalize by dividing each probability by the sum of probabilities\n",
    "            probs = probs / sum_probs\n",
    "\n",
    "        # Sample the next token using top-k sampling\n",
    "        # next_token = sample_top_k(probs, top_k)\n",
    "        # next_token = sample_top_k_top_p(probs, top_k, top_p)\n",
    "        next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        next_token = torch.where(finished_sequences.unsqueeze(-1), tokenizer.pad_token_id, next_token)\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "\n",
    "        # Extend the attention mask to include the newly generated token\n",
    "        new_attention_mask = torch.ones((attention_mask.shape[0], 1)).to(model.module.device)\n",
    "        attention_mask = torch.cat((attention_mask, new_attention_mask), dim=-1)\n",
    "        \n",
    "        finished_sequences |= next_token.squeeze(-1) == tokenizer.eos_token_id\n",
    "\n",
    "        # If all sequences are finished, break the loop\n",
    "        if finished_sequences.all():\n",
    "            break\n",
    "\n",
    "        # # Break if the model generates an end-of-sequence token\n",
    "        # if torch.all(next_token == tokenizer.eos_token_id):\n",
    "        #     break\n",
    "    return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(md, dat, tokenizer, batch_size=8, repetition_penalty=1.1, max_length=192 , bb_model = None):\n",
    "    # Put the model in evaluation mode\n",
    "\n",
    "    # DataLoader for batching the dataset\n",
    "    dataloader = DataLoader(dat, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and references\n",
    "    predictions = []\n",
    "    references = []\n",
    "    mrs = []\n",
    "    \n",
    "    b = 0\n",
    "    # Generate predictions and collect references in batches\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Get input IDs and attention mask from the batch and move to the device\n",
    "        # input_ids = torch.stack([torch.tensor(x, dtype=torch.long) for x in batch['input_ids']], dim=1).to(device)\n",
    "        # attention_mask = torch.stack([torch.tensor(x, dtype=torch.long) for x in batch['attention_mask']], dim=1).to(device)\n",
    "\n",
    "        input_ids = batch['input_ids'].to(md.device_ids[0])\n",
    "        attention_mask = batch['attention_mask'].to(md.device_ids[0])\n",
    "        # Generate predictions in batches with the model\n",
    "        with torch.no_grad():\n",
    "            # generated_ids = md.module.generate(input_ids=input_ids, \n",
    "            #                             attention_mask=attention_mask, \n",
    "            #                             max_length=max_length, \n",
    "            #                             pad_token_id=tokenizer.eos_token_id,\n",
    "            #                             bos_token_id=tokenizer.bos_token_id,\n",
    "            #                             eos_token_id=tokenizer.eos_token_id,\n",
    "            #                             repetition_penalty=repetition_penalty, \n",
    "            #                             do_sample = False, \n",
    "            #                             use_cache = True,\n",
    "            #                             length_penalty=1.0,\n",
    "            #                             early_stopping=False,\n",
    "            #                             num_beams=1,\n",
    "            #                             # renormalize_logits = True, \n",
    "            #                             output_logits = True,\n",
    "            #                             output_scores = True,\n",
    "            #                             return_dict_in_generate = True)\n",
    "            generated_ids = custom_generate(md, \n",
    "                                            input_ids=input_ids, \n",
    "                                            attention_mask=attention_mask, \n",
    "                                            max_length=max_length, \n",
    "                                            repetition_penalty=repetition_penalty, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            bb_model = bb_model)\n",
    "                                        \n",
    "#         # Decode generated predictions and references\n",
    "#         # for i in range(len(generated_ids.sequences)):\n",
    "#         for i in range(len(generated_ids)):\n",
    "#             # print(generated_ids[i])\n",
    "#             # generated_text = tokenizer.decode(generated_ids.sequences[i], skip_special_tokens=True)\n",
    "#             generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "\n",
    "#             # Append generated text and reference to lists\n",
    "#             predictions.append(generated_text)\n",
    "#         for mr in batch[\"meaning_representation\"]:\n",
    "#             references.append(meaning_to_references[mr])\n",
    "#             mrs.append(mr)\n",
    "#         b+=1\n",
    "#         # if(b==1):\n",
    "#         #     break\n",
    "#     return {'predictions': predictions, 'meaning_representation' : mrs, 'references': references}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs.\n",
      "Model is using the following GPUs: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "md = base_model\n",
    "md.eval()\n",
    "\n",
    "# # Move the model to the device\n",
    "md.to(device)\n",
    "\n",
    "# If multiple GPUs are available, wrap the model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
    "    md = torch.nn.DataParallel(md)\n",
    "    \n",
    "print(\"Model is using the following GPUs:\", md.device_ids)\n",
    "print(md.module.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/231 [03:18<2:29:54, 39.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# output = get_preds(model, tokenized_test_data, device, tokenizer)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mget_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 39\u001b[0m, in \u001b[0;36mget_preds\u001b[0;34m(md, dat, tokenizer, batch_size, repetition_penalty, max_length, bb_model)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Generate predictions in batches with the model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# generated_ids = md.module.generate(input_ids=input_ids, \u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#                             attention_mask=attention_mask, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#                             output_scores = True,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#                             return_dict_in_generate = True)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbb_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbb_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m, in \u001b[0;36mcustom_generate\u001b[0;34m(model, input_ids, attention_mask, max_length, repetition_penalty, tokenizer, top_k, temperature, top_p, bb_model)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m             position_ids \u001b[38;5;241m=\u001b[39m get_position_ids(input_ids\u001b[38;5;241m=\u001b[39mgenerated_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m---> 11\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m             logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Get logits of the last token\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         min_logit = torch.absolute(torch.min(logits)) + 1.0\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         logits += min_logit\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         # logits = apply_repetition_penalty(logits, generated_ids, repetition_penalty)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Convert logits to probabilities\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:189\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:134\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    132\u001b[0m module_indices[module] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_replicas):\n\u001b[0;32m--> 134\u001b[0m     replica \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_replicate_for_data_parallel()\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# This is a temporary fix for DDP. DDP needs to access the\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# replicated model parameters. It used to do so through\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# `mode.parameters()`. The fix added in #33907 for DP stops the\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# `parameters()` API from exposing the replicated parameters.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Hence, we add a `_former_parameters` dict here to support DDP.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     replica\u001b[38;5;241m.\u001b[39m_former_parameters \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:2528\u001b[0m, in \u001b[0;36mModule._replicate_for_data_parallel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2526\u001b[0m replica\u001b[38;5;241m.\u001b[39m_buffers \u001b[38;5;241m=\u001b[39m replica\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   2527\u001b[0m replica\u001b[38;5;241m.\u001b[39m_modules \u001b[38;5;241m=\u001b[39m replica\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 2528\u001b[0m \u001b[43mreplica\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_replica\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m replica\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1699\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1697\u001b[0m                 d\u001b[38;5;241m.\u001b[39mdiscard(name)\n\u001b[0;32m-> 1699\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_parameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Parameter):\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# output = get_preds(model, tokenized_test_data, device, tokenizer)\n",
    "output = get_preds(md, unique_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ft = get_preds(model_ft, unique_dataset, device, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs.\n",
      "50259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 168/231 [1:34:39<26:06, 24.87s/it]  "
     ]
    }
   ],
   "source": [
    "output_plugin_ft = get_preds(plugin_model_ft, unique_dataset, device, tokenizer, bb_model = model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_plugin_base = get_preds(plugin_model_base, unique_dataset, device, tokenizer, bb_model = base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sel_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Generate a natural language sentence from the following aspects: name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "Answer:\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "\n",
      "\n",
      "['A coffee shop in the city centre area called Blue Spice.', 'Blue Spice is a coffee shop in city centre.']\n"
     ]
    }
   ],
   "source": [
    "print(output['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output['meaning_representation'][sel_id])\n",
    "print('\\n')\n",
    "print(output['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Generate a natural language sentence from the following aspects: name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "Answer:\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "\n",
      "\n",
      "['A coffee shop in the city centre area called Blue Spice.', 'Blue Spice is a coffee shop in city centre.']\n"
     ]
    }
   ],
   "source": [
    "print(output_plugin_base['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_base['meaning_representation'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_base['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Generate a natural language sentence from the following aspects: name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "Answer: restaurant [restaurant in Portland OR]. customer rating(5 out of 5) nearby familyFriendly([yes]) nearby[Crowne Plaza Hotel](near Manchester United), close by airport (Burger King or Indian restaurants)[no]), priceRange (£20-25, £ cost per day); cashout method ($all customers receive discount), mobile phone range short to medium; nearByRainforest[Yes]; nearBYPass®[No], nearIndianRestaurants*[\"The Curry House\"]}, nearClose By The Sorrento Inn than other restaurants within walking distance. Price ranges between $ and more expensive than average. Nearby holidaymakers are Chinese tourists who on theirless experience.\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "\n",
      "\n",
      "['A coffee shop in the city centre area called Blue Spice.', 'Blue Spice is a coffee shop in city centre.']\n"
     ]
    }
   ],
   "source": [
    "print(output_ft['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_ft['meaning_representation'][sel_id])\n",
    "print('\\n')\n",
    "print(output_ft['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Generate a natural language sentence from the following aspects: name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "Answer: customer rating [5 out of 5 by Blue Curry Cafe])\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[coffee shop], area[city centre]\n",
      "\n",
      "\n",
      "['A coffee shop in the city centre area called Blue Spice.', 'Blue Spice is a coffee shop in city centre.']\n"
     ]
    }
   ],
   "source": [
    "print(output_plugin_ft['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_ft['meaning_representation'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_ft['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score = bleu_metric.compute(predictions=output['predictions'], references=output['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score = rouge_metric.compute(predictions=output['predictions'], references=output['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_plugin_base = bleu_metric.compute(predictions=output_plugin_base['predictions'], references=output_plugin_base['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_plugin_base = rouge_metric.compute(predictions=output_plugin_base['predictions'], references=output_plugin_base['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_base['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_ft = bleu_metric.compute(predictions=output_ft['predictions'], references=output_ft['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_ft = rouge_metric.compute(predictions=output_ft['predictions'], references=output_ft['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_plugin_ft = bleu_metric.compute(predictions=output_plugin_ft['predictions'], references=output_plugin_ft['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_plugin_ft = rouge_metric.compute(predictions=output_plugin_ft['predictions'], references=output_plugin_ft['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.042839042271541325\n",
      "ROUGE Score: {'rouge1': 0.47810644299968463, 'rouge2': 0.1967256342802732, 'rougeL': 0.3213933320725961, 'rougeLsum': 0.3229532168360921}\n",
      "BLEU Score: 0.04280878016611851\n",
      "ROUGE Score: {'rouge1': 0.4777149580688591, 'rouge2': 0.19657174706846953, 'rougeL': 0.3211130623944386, 'rougeLsum': 0.32290191831621473}\n",
      "BLEU Score: 0.02307265311729619\n",
      "ROUGE Score: {'rouge1': 0.3572597935500922, 'rouge2': 0.1332045881288912, 'rougeL': 0.22668951336918525, 'rougeLsum': 0.24761255318223108}\n",
      "BLEU Score: 0.022840266477484255\n",
      "ROUGE Score: {'rouge1': 0.35508644227244474, 'rouge2': 0.1323537554184298, 'rougeL': 0.2254154845352321, 'rougeLsum': 0.24691364292477058}\n"
     ]
    }
   ],
   "source": [
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score}\")\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_base['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_base}\")\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_ft}\")\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
