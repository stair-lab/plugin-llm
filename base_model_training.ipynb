{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, \n",
    "                          DataCollatorForLanguageModeling, EarlyStoppingCallback, get_linear_schedule_with_warmup, AdamW, \n",
    "                          TrainerCallback)\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "from peft import PeftModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROC_NAME = 'ft_model_5epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 64\n",
    "TARGET_SIZE = 128\n",
    "HYPER_TRAIN_SIZE = 0.2\n",
    "DATASET = 'e2e_nlg_cleaned'\n",
    "BASE_MODEL_NAME = 'gpt2-medium'\n",
    "EARLY_STOPPING_PATIENCE = 3 \n",
    "EARLY_STOPPING_THRESHOLD = 0.001\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "WARMUP_FRACTION = 0.1\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 8\n",
    "WEIGHT_DECAY = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the E2E NLG dataset from Hugging Face datasets library\n",
    "dataset = load_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33525, 33525)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']), len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Preprocess the dataset to include the meaning representation (MR) as input and human reference as target\n",
    "# def preprocess_function(examples):\n",
    "#     # Concatenate MR and human reference with a separator\n",
    "#     # inputs = [f\"<bos> {mr} <eos>\" for mr in examples[\"meaning_representation\"]]\n",
    "#     inputs = [f\"{mr}\" for mr in examples[\"meaning_representation\"]]\n",
    "#     # targets = [f\"<bos> {ref} <eos>\" for ref in examples[\"human_reference\"]]\n",
    "#     targets = [f\"{ref}\" for ref in examples[\"human_reference\"]]\n",
    "#     # print(inputs)\n",
    "#     # print(targets)\n",
    "#     model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "#     labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "#     # Replace padding token id's of the labels by -100 so that it's ignored by the loss\n",
    "#     labels[\"input_ids\"] = [\n",
    "#         [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq] \n",
    "#         for labels_seq in labels[\"input_ids\"]\n",
    "#     ]\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Extract the meaning representations (MR) and human references (target text) from the examples\n",
    "    inputs = examples[\"meaning_representation\"]\n",
    "    targets = examples[\"human_reference\"]\n",
    "    \n",
    "    # Tokenize the inputs (meaning representations)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=INPUT_SIZE, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        # return_tensors=\"pt\"  # Use numpy for batch processing\n",
    "    )\n",
    "    \n",
    "    # Tokenize the targets (human references)\n",
    "    tokenized_targets = tokenizer(\n",
    "        targets, \n",
    "        max_length=TARGET_SIZE, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        # return_tensors=\"pt\"  # Use numpy for batch processing\n",
    "    )\n",
    "    \n",
    "    # Concatenate input_ids (MR) and input_ids from the targets (human reference) into one sequence\n",
    "    # This creates the full sequence: [MR, target] (all tokenized)\n",
    "    concatenated_input_ids = [\n",
    "        list(input_seq) + list(target_seq) for input_seq, target_seq in zip(tokenized_inputs[\"input_ids\"], tokenized_targets[\"input_ids\"])\n",
    "    ]\n",
    "    \n",
    "    # Concatenate attention masks for both MR and target\n",
    "    concatenated_attention_mask = [\n",
    "        list(input_mask) + list(target_mask) for input_mask, target_mask in zip(tokenized_inputs[\"attention_mask\"], tokenized_targets[\"attention_mask\"])\n",
    "    ]\n",
    "    \n",
    "    # Prepare the labels for loss computation:\n",
    "    # We need to ignore the loss for the part corresponding to MR and only compute it for the target (human reference).\n",
    "    \n",
    "    labels = []\n",
    "    for input_len, target_seq in zip([INPUT_SIZE] * len(inputs), tokenized_targets[\"input_ids\"]):\n",
    "        # Ignore loss for MR part by setting it to -100\n",
    "        labels_seq = [-100] * input_len\n",
    "        \n",
    "        # For the target sequence, we keep the tokens, but set padding tokens to -100\n",
    "        labels_seq += [token if token != tokenizer.pad_token_id else -100 for token in target_seq]\n",
    "        \n",
    "        labels.append(labels_seq)\n",
    "    \n",
    "    # Return the final dictionary containing input_ids, attention_mask, and labels\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(concatenated_input_ids),\n",
    "        \"attention_mask\": torch.tensor(concatenated_attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"meaning_representation\"], text_target=examples[\"human_reference\"], padding=\"max_length\", \n",
    "                     truncation=True, max_length=TARGET_SIZE, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33525/33525 [00:07<00:00, 4260.26 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4299/4299 [00:01<00:00, 3641.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4693/4693 [00:01<00:00, 4152.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to the dataset\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, \n",
    "                                # remove_columns=[\"meaning_representation\", \"human_reference\"]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset['train'], tokenized_dataset['hypervalidation'] = tokenized_dataset['train'].train_test_split(\n",
    "#     test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the tokenized dataset to a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(tokenized_dataset['train'])\n",
    "\n",
    "# Step 1: Identify unique MRs and group by MR\n",
    "grouped_by_mr = df.groupby('meaning_representation')\n",
    "\n",
    "# Step 2: Extract all unique MRs\n",
    "unique_mrs = df['meaning_representation'].unique()\n",
    "\n",
    "# Step 3: Perform train-test split on the unique MRs\n",
    "train_mrs, hypervalidation_mrs = train_test_split(unique_mrs, test_size=HYPER_TRAIN_SIZE, random_state=42)\n",
    "\n",
    "# Step 4: Create new DataFrames for train and hypervalidation based on the split MRs\n",
    "train_df = df[df['meaning_representation'].isin(train_mrs)]\n",
    "hypervalidation_df = df[df['meaning_representation'].isin(hypervalidation_mrs)]\n",
    "\n",
    "# Step 5: Convert back to the Dataset format for Hugging Face\n",
    "train_dataset = DatasetDict({\"train\": Dataset.from_pandas(train_df)})\n",
    "hypervalidation_dataset = DatasetDict({\"hypervalidation\": Dataset.from_pandas(hypervalidation_df)})\n",
    "\n",
    "tokenized_dataset = {}\n",
    "\n",
    "# Update tokenized_dataset with the new split\n",
    "tokenized_dataset['train'] = train_dataset['train'].remove_columns([\"meaning_representation\", \"human_reference\", \"__index_level_0__\"])\n",
    "\n",
    "tokenized_dataset['hypervalidation'] = hypervalidation_dataset['hypervalidation'].remove_columns([\"meaning_representation\", \"human_reference\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data collator for language modeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # PEFT LoRA Configuration\n",
    "# lora_config = LoraConfig(\n",
    "#     # task_type=TaskType.CAUSAL_LM,  # Type of task\n",
    "#     r=8,                           # Low-rank dimension\n",
    "#     lora_alpha=32,                 # Scaling factor\n",
    "#     lora_dropout=0.1,              # Dropout\n",
    "#     target_modules=[\"c_attn\", \"q_attn\", \"v_attn\"],  # GPT-2 target modules for LoRA\n",
    "#     # target_modules=[\"c_attn\"],  # GPT-2 target modules for LoRA\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply PEFT LoRA to the GPT-2 model\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if(param.requires_grad):\n",
    "#         print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrintPredictionsCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model=None, tokenizer=None, eval_dataloader=None, **kwargs):\n",
    "        # Generate a few predictions\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "            # Generate predictions\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(inputs, attention_mask=attention_mask, max_length=256)\n",
    "\n",
    "            # Filter out invalid token IDs and padding (-100) from inputs and predictions\n",
    "            def safe_decode(token_ids):\n",
    "                # Filter out invalid token IDs (e.g., -100) before decoding\n",
    "                # print(token_ids)\n",
    "                valid_token_ids = [token_id for token_id in token_ids if 0 <= token_id < tokenizer.vocab_size]\n",
    "                return tokenizer.decode(valid_token_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Decode the input, predictions, and true references\n",
    "            inputs_decoded = [safe_decode(input_ids) for input_ids in inputs]\n",
    "            preds_decoded = [safe_decode(generated_id) for generated_id in generated_ids]\n",
    "            refs_decoded = [safe_decode(ref) for ref in batch[\"labels\"]]\n",
    "\n",
    "            # Print out the input, prediction, and true reference\n",
    "            for i in range(min(3, len(inputs_decoded))):  # Print up to 3 samples per evaluation\n",
    "                print(f\"\\nInput (MR): {inputs_decoded[i]}\")\n",
    "                print(f\"Prediction: {preds_decoded[i]}\")\n",
    "                print(f\"Reference: {refs_decoded[i]}\")\n",
    "            \n",
    "            break  # Remove this to print for every batch during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{PROC_NAME}',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # This batch size is per GPU\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir=f'./logs/{PROC_NAME}',\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    # fp16=True,  # Enable mixed-precision training for faster training\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,  # Required for early stopping\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to determine the best model (optional)\n",
    "    greater_is_better=False,  # Set to False if lower metric is better (e.g., loss)\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Accelerator\n",
    "# accelerator = Accelerator()\n",
    "# device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accelerator for multi-GPU support\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['hypervalidation']\n",
    "\n",
    "# peft_model, train_dataset, eval_dataset = accelerator.prepare(peft_model, train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "total_steps = len(train_dataset) * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(WARMUP_FRACTION * total_steps),  # Warm-up for the first 10% of steps\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                       # The model with PEFT applied\n",
    "    args=training_args,                     # Training arguments\n",
    "    train_dataset=train_dataset, # Training data\n",
    "    eval_dataset=eval_dataset, # Validation data\n",
    "    # data_collator=data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    optimizers=(optimizer, scheduler),  # Pass optimizer and scheduler\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE, \n",
    "                                     early_stopping_threshold=EARLY_STOPPING_THRESHOLD), \n",
    "               PrintPredictionsCallback()]  # Add early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2095' max='2095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2095/2095 19:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.231900</td>\n",
       "      <td>2.522599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.538700</td>\n",
       "      <td>1.466495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.313167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.248600</td>\n",
       "      <td>1.235538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.235800</td>\n",
       "      <td>1.189717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input (MR): name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "Prediction: name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food. It is a coffee shop with a family friendly atmosphere and is near the riverside. It is a family friendly coffee shop with a family friendly atmosphere and is near the riverside. It is a family friendly coffee shop with a family friendly atmosphere and is near the riverside. It is a family friendly coffee shop with a\n",
      "Reference: The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "\n",
      "Input (MR): name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "Prediction: name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area. It is located in the Riverside area of San Diego. It is a restaurant with a high customer rating. It has a high customer rating. It has a high customer rating. It has a high customer rating. It has a high customer rating. It has a high customer rating. It has a high customer rating. It\n",
      "Reference: The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "\n",
      "Input (MR): name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n",
      "Prediction: name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25. It is located in Cambridge, Cambridge, Cambridge, Cambridge. It is a restaurant with a customer rating of 3 out of 5 and a price range of Â£20-Â£25. It is located in Cambridge, Cambridge, Cambridge. It is a restaurant with a customer rating of 3 out of 5 and a price range\n",
      "Reference: Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input (MR): name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "Prediction: name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food. It is located in the riverside area. It is not family friendly. It is not family friendly. It is not family friendly. It is not family friendly. It is not family friendly. It is not family friendly. It is not family friendly. It is not family friendly. It is not family friendly. It\n",
      "Reference: The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "\n",
      "Input (MR): name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "Prediction: name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area. It is a French restaurant with a high customer rating. It is located in the riverside area. It has a 5 out of 5 rating. It is a French restaurant. It is located in the Riverside area. It has a 5 out of 5 rating. It is a French restaurant. It is located in the Riverside\n",
      "Reference: The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "\n",
      "Input (MR): name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n",
      "Prediction: name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25. It is located in Cambridge and has a 3 star rating. It is a restaurant that serves Chinese food and has a 3 star rating. It is located in Cambridge and has a 3 star rating. It is a restaurant that has a 3 star rating. It is located in Cambridge and has a 3 star rating. It\n",
      "Reference: Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input (MR): name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "Prediction: name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food. It is located in the riverside area. It is located in the Riverside area. It is a coffee shop with a low customer rating. It is located in the riverside area. It is not family friendly. It is not family friendly. It is located in the Riverside area. It is not family friendly. It\n",
      "Reference: The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "\n",
      "Input (MR): name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "Prediction: name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.  It is a 5 star restaurant with a high customer rating.  It is located in the riverside area.  It is a French restaurant with a high price range.  It is located in the Riverside area.  It is a 5 star restaurant.  It is a French restaurant.  It is located in\n",
      "Reference: The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "\n",
      "Input (MR): name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n",
      "Prediction: name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25. It is located in Cambridge and has a 3 star rating. It is a restaurant that serves Indian food. It is located in the city centre and has a 3 star rating. It is a restaurant that serves Indian food. It is located in the city centre and has a price range of Â£20-Â£25.\n",
      "Reference: Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input (MR): name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "Prediction: name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food. It is located in the Riverside area. It is located near Burger King. It is located in the riverside area. It is a coffee shop that serves Japanese food. It is not family friendly. It has a low customer rating. It is located near Burger King. It is located near Burger King. It is near\n",
      "Reference: The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "\n",
      "Input (MR): name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "Prediction: name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.  It is a restaurant that serves French food.  It is located in the riverside area.  It is a 5 star restaurant.  It is a French restaurant.  It is located in the riverside area.  It is a 5 star restaurant.  It is a French restaurant.  It is located\n",
      "Reference: The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "\n",
      "Input (MR): name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n",
      "Prediction: name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25. It is a restaurant that serves Japanese food. It is located in Cambridge and is a restaurant that serves Japanese food. It is located in the city centre. It is a restaurant that is not family friendly. It is located in the city centre. It is a restaurant that is not family friendly. It is called Taste\n",
      "Reference: Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input (MR): name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "Prediction: name[The Eagle], eatType[coffee shop], food[Japanese], priceRange[less than Â£20], customer rating[low], area[riverside], familyFriendly[yes], near[Burger King]The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food. It is located in the Riverside area. It is a coffee shop that serves Japanese food. It is located near Burger King. It has a customer rating of 1 out of 5. It is located near Burger King. It is near Burger King. It is near Burger King. It is near Burger King. It has a\n",
      "Reference: The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than Â£20 for Japanese food.\n",
      "\n",
      "Input (MR): name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "Prediction: name[The Rice Boat], eatType[restaurant], food[French], customer rating[5 out of 5], area[riverside]The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.  It is a 5 star restaurant with a price range of Â£20-Â£25.  It is located in the riverside area.  It is a restaurant that serves French food.  It is a restaurant that is not kid friendly.  It is located in the riverside area.  It is a restaurant\n",
      "Reference: The Rice Boat is an adult French restaurant with high customer rating  located in the Riverside area.\n",
      "\n",
      "Input (MR): name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n",
      "Prediction: name[Taste of Cambridge], eatType[restaurant], customer rating[3 out of 5]Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25. It is a restaurant that serves Japanese food. It is located in Cambridge. It is a restaurant that serves Japanese food. It is a restaurant that is not a family friendly restaurant. It is located in the city centre. It is a restaurant that is not a family friendly restaurant. It is a restaurant that is not\n",
      "Reference: Taste of Cambridge is a restaurant with a customer rating of 3 out of 5 and and a price range of Â£20-Â£25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2095, training_loss=2.5584196131667545, metrics={'train_runtime': 1153.6363, 'train_samples_per_second': 115.964, 'train_steps_per_second': 1.816, 'total_flos': 4.659059209273344e+16, 'train_loss': 2.5584196131667545, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accelerator.wait_for_everyone()  # Synchronize GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/ft_model_5epochs/tokenizer_config.json',\n",
       " './models/ft_model_5epochs/special_tokens_map.json',\n",
       " './models/ft_model_5epochs/vocab.json',\n",
       " './models/ft_model_5epochs/merges.txt',\n",
       " './models/ft_model_5epochs/added_tokens.json',\n",
       " './models/ft_model_5epochs/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "# peft_model.save_pretrained(f'./models/{PROC_NAME}')\n",
    "model.save_pretrained(f'./models/{PROC_NAME}')\n",
    "tokenizer.save_pretrained(f'./models/{PROC_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
