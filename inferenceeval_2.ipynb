{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "import re\n",
    "# from peft import PeftModel\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from nlgmetricverse import NLGMetricverse, load_metric\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/decoding/')\n",
    "\n",
    "from utils.data_preprocess import process_e2e_nlg_cleaned, process_common_gen, process_web_nlg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"common_gen\"\n",
    "# DATASET_NAME = \"web_nlg\"\n",
    "# DATASET_NAME = \"e2e_nlg_cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BASE_PROC_NAME = 'meta-llama/Llama-3.1-8B'\n",
    "# BASE_PROC_NAME = 'gpt2-medium'\n",
    "BASE_PROC_NAME = 'gpt2-xl'\n",
    "MODEL_TYPE = 'gpt2'\n",
    "FT_PROC_NAME = 'ft_text_completion_e2e_nlg_cleaned_gpt2-medium_5_5e-05_8_0.01_42'\n",
    "PLUGIN_FT_PROC_NAME = 'plugin_over_ft_text_completion_e2e_nlg_cleaned_ft_text_completion_e2e_nlg_cleaned_gpt2-medium_5_5e-05_8_0.01_42_80_5e-06_8_1.0_42'\n",
    "PLUGIN_BASE_PROC_NAME = 'weighted_gpt2_e2e_nlg_cleaned_5_5e-05_4_10.0_0.5_42_gpt2-medium'\n",
    "\n",
    "\n",
    "\n",
    "ACCESS_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_CONTEXT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 128\n",
    "TARGET_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(f'./models/{PLUGIN_BASE_PROC_NAME}')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_PROC_NAME)\n",
    "print(len(tokenizer.vocab))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LEN_CONTEXT > 0):\n",
    "    INPUT_SIZE += LEN_CONTEXT*(INPUT_SIZE + TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "if(MODEL_TYPE == 'llama'):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_PROC_NAME, token = ACCESS_TOKEN, torch_dtype=torch.float16)\n",
    "else:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_PROC_NAME)\n",
    "# base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# base_model_for_loading = AutoModelForCausalLM.from_pretrained(BASE_PROC_NAME)\n",
    "# base_model_for_loading.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_ft = PeftModel.from_pretrained(base_model_for_loading, f\"./models/{FT_PROC_NAME}\")\n",
    "# model_ft.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# model_ft = AutoModelForCausalLM.from_pretrained(f\"./models/{FT_PROC_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plugin_model_ft = AutoModelForCausalLM.from_pretrained(f\"./models/{PLUGIN_FT_PROC_NAME}\") \n",
    "# plugin_model_base = AutoModelForCausalLM.from_pretrained(f\"./models/{PLUGIN_BASE_PROC_NAME}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2026/2026 [00:00<00:00, 28003.68 examples/s]\n",
      "Map: 100%|██████████| 1992/1992 [00:00<00:00, 28944.47 examples/s]\n",
      "Map: 100%|██████████| 1476/1476 [00:00<00:00, 1506.51 examples/s]\n",
      "Map: 100%|██████████| 2026/2026 [00:00<00:00, 2062.84 examples/s]\n",
      "Map: 100%|██████████| 1992/1992 [00:00<00:00, 2026.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train data: 1476\n",
      "length of validation data: 2026\n",
      "length of test data: 1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " # loading data\n",
    "from src.processed_dataset import ProcessedDataset\n",
    "dataset = ProcessedDataset(name=DATASET_NAME, base_model_name=BASE_PROC_NAME)\n",
    "\n",
    "# processing data\n",
    "dataset.mapped_tokenize(tokenizer=tokenizer, input_size=INPUT_SIZE, \n",
    "                        target_size=TARGET_SIZE)\n",
    "\n",
    "for ke in dataset.data.keys():\n",
    "    print(f'length of {ke} data: {len(dataset.data[ke])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s = 0\n",
    "# # for t in dataset['validation']['meaning_representation']:\n",
    "# #     if(t in dataset['test']['meaning_representation']):\n",
    "# #         print(t)\n",
    "# #         s+=1\n",
    "# # print(s)\n",
    "\n",
    "# train_d = dataset['train'].to_pandas()\n",
    "# val_d = dataset['validation'].to_pandas()\n",
    "# test_d = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(len(val_d))\n",
    "# # print(len(test_d))\n",
    "# len(test_d)\n",
    "# # val_d.merge(test_d, on = 'meaning_representation', how = 'inner')\n",
    "# len(train_d['meaning_representation'].unique()), len(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load test dataset (replace with your dataset)\n",
    "# # dataset = load_dataset(\"e2e_nlg_cleaned\", split=\"test\")\n",
    "# if(DATASET_NAME == 'web_nlg'):\n",
    "#     dataset = load_dataset(DATASET_NAME, 'webnlg_challenge_2017', trust_remote_code=True)\n",
    "# else:\n",
    "#     dataset = load_dataset(DATASET_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(DATASET_NAME == 'e2e_nlg_cleaned'):\n",
    "#     dataset = process_e2e_nlg_cleaned(dataset, BASE_PROC_NAME)\n",
    "# elif(DATASET_NAME == 'common_gen'):\n",
    "#     dataset = process_common_gen(dataset, BASE_PROC_NAME)\n",
    "# elif(DATASET_NAME == 'web_nlg'):\n",
    "#     dataset = process_web_nlg(dataset, BASE_PROC_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_preconcat_input_target(examples, tokenizer, \n",
    "#                                         input_size, target_size):\n",
    "#     # Extract the meaning representations (MR) and human references (target text) from the examples\n",
    "#     inputs = examples[\"meaning_representation\"]\n",
    "#     targets = examples[\"human_reference\"]\n",
    "    \n",
    "#     # Tokenize the targets (human references)\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         inputs, add_special_tokens=True, max_length = INPUT_SIZE, truncation=True, \n",
    "#     )\n",
    "\n",
    "#     # Tokenize the targets (human references)\n",
    "#     tokenized_targets = tokenizer(\n",
    "#         targets, add_special_tokens=False, max_length = TARGET_SIZE, truncation=True, \n",
    "#     )\n",
    "\n",
    "#     tokenized_sentences_inp_ids = []\n",
    "#     for inp, tar in zip(tokenized_inputs['input_ids'], tokenized_targets['input_ids']):\n",
    "#         n_pads = (INPUT_SIZE + TARGET_SIZE) - (len(inp) + len(tar))\n",
    "#         tokenized_sentence_inp_ids = [tokenizer.eos_token_id]*n_pads + inp + tar\n",
    "#         tokenized_sentences_inp_ids.append(tokenized_sentence_inp_ids)\n",
    "    \n",
    "#     tokenized_sentences_att_mask = []\n",
    "#     for inp, tar in zip(tokenized_inputs['attention_mask'], tokenized_targets['attention_mask']):\n",
    "#         n_pads = (INPUT_SIZE + TARGET_SIZE) - (len(inp) + len(tar))\n",
    "#         tokenized_sentence_att_mask = [0]*n_pads + inp + tar\n",
    "#         tokenized_sentences_att_mask.append(tokenized_sentence_att_mask)\n",
    "    \n",
    "#     tokenized_sentences = {}\n",
    "#     tokenized_sentences['input_ids'] = tokenized_sentences_inp_ids\n",
    "#     tokenized_sentences['attention_mask'] = tokenized_sentences_att_mask\n",
    "\n",
    "#     # sentences = []\n",
    "#     # for inp, tar in zip(inputs, targets):\n",
    "#     #     sentences.append(inp + tar)\n",
    "\n",
    "#     # # Tokenize the inputs (meaning representations)\n",
    "#     # tokenized_sentences = tokenizer(\n",
    "#     #     sentences, \n",
    "#     #     max_length=input_size + target_size, \n",
    "#     #     truncation=True, \n",
    "#     #     padding=\"max_length\", \n",
    "#     #     # return_tensors=\"pt\"  # Use numpy for batch processing\n",
    "#     # )\n",
    "        \n",
    "#     labels = []\n",
    "#     for comb_seq, target_seq in zip(tokenized_sentences['input_ids'], tokenized_targets['input_ids']):\n",
    "#         label_seq = [-100]*len(comb_seq)\n",
    "#         label_seq[-len(target_seq):] = target_seq\n",
    "#         labels.append(label_seq)\n",
    "        \n",
    "    \n",
    "#     # Return the final dictionary containing input_ids, attention_mask, and labels\n",
    "#     return {\n",
    "#         \"input_ids\": torch.tensor(tokenized_sentences['input_ids']),\n",
    "#         \"attention_mask\": torch.tensor(tokenized_sentences['attention_mask']),\n",
    "#         \"labels\": torch.tensor(labels)\n",
    "#     }\n",
    "\n",
    "# def mapped_tokenize(dataset, tokenizer, input_size, target_size):\n",
    "#     fn_kwargs_dict={\"tokenizer\": tokenizer, \"input_size\": input_size, 'target_size' : target_size}\n",
    "#     # this is padding, mr, padding, hr\n",
    "#     # self.data = self.data.map(self.preprocess_concat_input_target, \n",
    "#     #                           batched=True, \n",
    "#     #                           fn_kwargs=fn_kwargs_dict,\n",
    "#     #                             # remove_columns=[\"meaning_representation\", \"human_reference\"]\n",
    "#     #                         )\n",
    "#     # this is padding, mr, hr\n",
    "#     dataset = dataset.map(preprocess_preconcat_input_target, \n",
    "#                                 batched=True, \n",
    "#                                 fn_kwargs=fn_kwargs_dict,\n",
    "#                                 # remove_columns=[\"meaning_representation\", \"human_reference\"]\n",
    "#                             )\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = mapped_tokenize(dataset, tokenizer, INPUT_SIZE, TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 582, 3332, 319, 262, 4314, 355, 262, 3056, 19036, 503, 286, 262, 18757, 13, 50256]\n",
      "[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 3198, 6827, 326, 3544, 477, 262, 1708, 10838, 25, 12797, 11, 1650, 11, 3056, 11, 4314, 11, 18757, 11, 318, 25, 220, 32, 582, 3332, 319, 262, 4314, 355, 262, 3056, 19036, 503, 286, 262, 18757, 13, 50256]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1\n",
      "50256\n",
      "50256\n",
      "True\n",
      "True\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "sel_id = 0\n",
    "set_type = 'test'\n",
    "\n",
    "s = 0\n",
    "for ip in dataset[set_type][sel_id]['input_ids']:\n",
    "    if(ip != tokenizer.pad_token_id):\n",
    "        s += 1\n",
    "test_l = []\n",
    "for l in dataset[set_type][sel_id]['labels']:\n",
    "    if(l != -100):\n",
    "        test_l.append(l)\n",
    "print(test_l)\n",
    "print(dataset[set_type][sel_id]['input_ids'])\n",
    "print(dataset[set_type][sel_id]['attention_mask'])\n",
    "print(dataset[set_type][sel_id]['attention_mask'][-1])\n",
    "print(dataset[set_type][sel_id]['input_ids'][-1])\n",
    "print(dataset[set_type][sel_id]['labels'][-1])\n",
    "print((sum(dataset[set_type][sel_id]['attention_mask']) - s) == 1)\n",
    "print(dataset[set_type][sel_id]['input_ids'][-len(test_l):] == test_l)\n",
    "print(dataset[set_type][sel_id]['input_ids'][-len(test_l)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(test_l))\n",
    "# print(tokenizer.decode(dataset[set_type][sel_id]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plugin_model_ft.resize_token_embeddings(len(tokenizer))\n",
    "# plugin_model_base.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load test dataset (replace with your dataset)\n",
    "# # dataset = load_dataset(\"e2e_nlg_cleaned\", split=\"test\")\n",
    "# if(DATASET_NAME == 'web_nlg'):\n",
    "#     dataset = load_dataset(DATASET_NAME, 'webnlg_challenge_2017', trust_remote_code=True)\n",
    "# else:\n",
    "#     dataset = load_dataset(DATASET_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(set(dataset['train']['category']))\n",
    "# print(set(dataset['dev']['category']))\n",
    "# print(set(dataset['test']['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_categories = ['Airport', 'Building', 'University', 'Monument', 'MeanOfTransportation'] \n",
    "# test_categories = ['Artist', 'Politician', 'Athlete', 'ComicsCharacter', 'Astronaut', 'SportsTeam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Filter the 'train', 'dev', and 'test' datasets by category\n",
    "# def filter_by_category(example, categories):\n",
    "#     return example['category'] in categories\n",
    "\n",
    "# # Filter the 'train' set to only include the \"Airport\" category\n",
    "# train_dataset = dataset['train'].filter(lambda x: filter_by_category(x, train_categories))\n",
    "\n",
    "# # # Filter 'dev' and 'test' sets to only include the \"Food\" category\n",
    "# dev_dataset = dataset['dev'].filter(lambda x: filter_by_category(x, test_categories))\n",
    "# test_dataset = dataset['test'].filter(lambda x: filter_by_category(x, test_categories))\n",
    "\n",
    "# # # Step 3: Select only one reference sentence from 'text' field based on 'comment'\n",
    "# def select_good_comment(example):\n",
    "#     for i, comment in enumerate(example['lex']['comment']):\n",
    "#         if comment == 'good':\n",
    "#             return {'human_reference': example['lex']['text'][i]}  # Pick the sentence marked 'good'\n",
    "#     return {'human_reference': ''}  # Default to the first sentence if none are marked 'good'\n",
    "# train_dataset = train_dataset.map(select_good_comment)\n",
    "# dev_dataset = dev_dataset.map(select_good_comment)\n",
    "# test_dataset = test_dataset.map(select_good_comment)\n",
    "\n",
    "# def join_mtriple_set(example):\n",
    "#     triples = example['modified_triple_sets']['mtriple_set'][0]\n",
    "#     prompt = ''\n",
    "#     for i, triple in enumerate(triples, start=1):\n",
    "#         prompt += f\"{triple}\\n\"\n",
    "#     # return {'meaning_representation': '\\n'.join(example['modified_triple_sets']['mtriple_set'][0])}\n",
    "#     return {'meaning_representation': prompt}\n",
    "\n",
    "# # Apply the function to each dataset\n",
    "# train_dataset = train_dataset.map(join_mtriple_set)\n",
    "# dev_dataset = dev_dataset.map(join_mtriple_set)\n",
    "# test_dataset = test_dataset.map(join_mtriple_set)\n",
    "\n",
    "\n",
    "# print(type(dev_dataset))\n",
    "\n",
    "# combined_dev_test = concatenate_datasets([dev_dataset, test_dataset])\n",
    "# print(len(dev_dataset), len(test_dataset), len(combined_dev_test))\n",
    "\n",
    "# df = combined_dev_test.to_pandas()\n",
    "\n",
    "# # Shuffle the DataFrame to ensure randomness\n",
    "# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Step 3: Split unique \"meaning_representation\" values\n",
    "# unique_meaning_reps = df[\"meaning_representation\"].unique()\n",
    "# split_point = len(unique_meaning_reps) // 2\n",
    "\n",
    "# # Split unique values into two groups\n",
    "# group1_meaning_reps = set(unique_meaning_reps[:split_point])\n",
    "# group2_meaning_reps = set(unique_meaning_reps[split_point:])\n",
    "\n",
    "# # Step 4: Assign rows to each split based on \"meaning_representation\" value\n",
    "# group1_df = df[df[\"meaning_representation\"].isin(group1_meaning_reps)]\n",
    "# group2_df = df[df[\"meaning_representation\"].isin(group2_meaning_reps)]\n",
    "\n",
    "# # Step 5: Convert back to `datasets.Dataset` format\n",
    "# group1_dataset = Dataset.from_pandas(group1_df)\n",
    "# group2_dataset = Dataset.from_pandas(group2_df)\n",
    "\n",
    "# # Step 6: Create a DatasetDict to hold the two splits\n",
    "# combined_dev_test = DatasetDict({\n",
    "#     \"validation\": group1_dataset,\n",
    "#     \"test\": group2_dataset\n",
    "# })\n",
    "\n",
    "# # combined_dev_test = combined_dev_test.train_test_split(test_size=0.5, seed=42)\n",
    "# # print(len(combined_dev_test['train']), len(combined_dev_test['test']))\n",
    "# # # Rename the splits for clarity\n",
    "# # combined_dev_test = DatasetDict({\n",
    "# #     'validation': combined_dev_test['train'],  # Rename 'train' split as 'validation'\n",
    "# #     'test': combined_dev_test['test']  # Keep 'test' split as 'test'\n",
    "# # })\n",
    "# # print(len(combined_dev_test['validation']), len(combined_dev_test['test']))\n",
    "\n",
    "\n",
    "# combined_dev_test['validation'] = combined_dev_test['validation'].map(select_good_comment)\n",
    "# combined_dev_test['test'] = combined_dev_test['test'].map(select_good_comment)\n",
    "\n",
    "\n",
    "# combined_dev_test['validation'] = combined_dev_test['validation'].map(join_mtriple_set)\n",
    "# combined_dev_test['test'] = combined_dev_test['test'].map(join_mtriple_set)\n",
    "\n",
    "\n",
    "# # print(len(combined_dev_test['validation']), len(combined_dev_test['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(DATASET_NAME == 'e2e_nlg_cleaned'):\n",
    "#     dataset = process_e2e_nlg_cleaned(dataset, BASE_PROC_NAME)\n",
    "# elif(DATASET_NAME == 'common_gen'):\n",
    "#     dataset = process_common_gen(dataset, BASE_PROC_NAME)\n",
    "# elif(DATASET_NAME == 'web_nlg'):\n",
    "#     dataset = process_web_nlg(dataset, BASE_PROC_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1476\n",
      "{'meaning_representation': 'One sentence that uses all the following concepts: beach, row, hut, is: ', 'human_reference': 'a man is seen skateboarding past a row of beach huts beside the beach', 'input_ids': [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 3198, 6827, 326, 3544, 477, 262, 1708, 10838, 25, 10481, 11, 5752, 11, 40812, 11, 318, 25, 220, 64, 582, 318, 1775, 22647, 27794, 1613, 257, 5752, 286, 10481, 289, 5500, 13970, 262, 10481, 50256], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 64, 582, 318, 1775, 22647, 27794, 1613, 257, 5752, 286, 10481, 289, 5500, 13970, 262, 10481, 50256]}\n",
      "validation 2026\n",
      "{'meaning_representation': 'One sentence that uses all the following concepts: rock, skip, kid, is: ', 'human_reference': 'The kid skipped a rock at the lake.', 'input_ids': [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 3198, 6827, 326, 3544, 477, 262, 1708, 10838, 25, 3881, 11, 14267, 11, 5141, 11, 318, 25, 220, 464, 5141, 26684, 257, 3881, 379, 262, 13546, 13, 50256], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 464, 5141, 26684, 257, 3881, 379, 262, 13546, 13, 50256]}\n",
      "test 1992\n",
      "{'meaning_representation': 'One sentence that uses all the following concepts: worker, shovel, examine, hold, field, is: ', 'human_reference': 'a worker holds a shovel in his hand while examining his field of vegetables', 'input_ids': [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 3198, 6827, 326, 3544, 477, 262, 1708, 10838, 25, 8383, 11, 33677, 11, 10716, 11, 1745, 11, 2214, 11, 318, 25, 220, 64, 8383, 6622, 257, 33677, 287, 465, 1021, 981, 17247, 465, 2214, 286, 13701, 50256], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 64, 8383, 6622, 257, 33677, 287, 465, 1021, 981, 17247, 465, 2214, 286, 13701, 50256]}\n"
     ]
    }
   ],
   "source": [
    "for d in ['train', 'validation', 'test']:\n",
    "    print(d, len(dataset[d]))\n",
    "    print(dataset[d][220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sentence that uses all the following concepts: cut, knife, kitchen, is: \n",
      "cutting fresh raw beef with knife in the kitchen\n",
      "\n",
      "One sentence that uses all the following concepts: work, machine, wear, is: \n",
      "I wear protective glasses while working on the machine.\n",
      "\n",
      "One sentence that uses all the following concepts: wear, tractor, drive, is: \n",
      "The farmer wore a jacket while driving the tractor.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\n",
    "if(DATASET_NAME == 'e2e_nlg_cleaned'):\n",
    "    if(LEN_CONTEXT > 0):\n",
    "        np.random.seed(42)\n",
    "        ic_ids = np.random.choice(len(dataset['validation']), LEN_CONTEXT)\n",
    "        if(BASE_PROC_NAME == 'gpt2-medium'):\n",
    "            context += \"<examples>\\n\"\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n'\n",
    "                )\n",
    "            context += \"</examples>\\n\"\n",
    "        elif(BASE_PROC_NAME == 'gpt2-xl'):\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'] + \n",
    "                    '\\n' + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "        elif('Llama-3.1-8B' in BASE_PROC_NAME):\n",
    "            context += \"Consider the following examples of restaurant descriptions from attributes.\\n\\n<examples>\\n\\n\"\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                static_str_from_prompt = 'Do not provide explanation. Just convert the following attributes of a restaurant in a coherent sentence.\\n\\n'\n",
    "                context += f'Example {j+1} --\\n'\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'][len(static_str_from_prompt):] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "            context += \"</examples>\\n\"\n",
    "elif(DATASET_NAME == 'web_nlg'):\n",
    "    if(LEN_CONTEXT > 0):\n",
    "        np.random.seed(42)\n",
    "        ic_ids = np.random.choice(len(dataset['validation']), LEN_CONTEXT)\n",
    "        if(BASE_PROC_NAME == 'gpt2-medium'):\n",
    "            context += \"Consider the following examples of entity descriptions from facts.\\n\\n<examples>\\n\\n\"\n",
    "            # context += \"<examples>\\n\\n\"\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                static_str_from_prompt = 'Convert the following facts into a coherent sentence:\\n\\n'\n",
    "                context += f'Example {j+1} --\\n'\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'][len(static_str_from_prompt):] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "            context += \"</examples>\\n\"\n",
    "        elif(BASE_PROC_NAME == 'gpt2-xl'):\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "        elif('Llama-3.1-8B' in BASE_PROC_NAME):\n",
    "            context += \"Consider the following examples of entity descriptions from facts.\\n\\n<examples>\\n\\n\"\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                static_str_from_prompt = 'Do not provide explanation or follow-up. Just convert the following facts of an entity into a coherent sentence.\\n\\n'\n",
    "                context += f'Example {j+1} --\\n'\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'][len(static_str_from_prompt):] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "            context += \"</examples>\\n\"\n",
    "elif(DATASET_NAME == 'common_gen'):\n",
    "    if(LEN_CONTEXT > 0):\n",
    "        np.random.seed(42)\n",
    "        ic_ids = np.random.choice(len(dataset['validation']), LEN_CONTEXT)\n",
    "        if(BASE_PROC_NAME == 'gpt2-medium'):\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                context += (                    \n",
    "                    dataset['validation'][int(i)]['meaning_representation'] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "        elif(BASE_PROC_NAME == 'gpt2-xl'):\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                context += (                    \n",
    "                    dataset['validation'][int(i)]['meaning_representation'] + \n",
    "                    '\\n' + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "            # context += 'Complete the sentence just like above examples.\\n\\n'\n",
    "        elif('Llama-3.1-8B' in BASE_PROC_NAME):\n",
    "            # context += \"Consider the following examples of coherent sentences from concepts.\\n\\n<examples>\\n\\n\"\n",
    "            for j, i in enumerate(ic_ids):\n",
    "                static_str_from_prompt = 'Do not provide explanation. Just generate a single coherent sentence based on the following concepts.\\n\\n'\n",
    "                # static_str_from_prompt = ''\n",
    "                # context += f'Example {j+1} --\\n'\n",
    "                context += (\n",
    "                    dataset['validation'][int(i)]['meaning_representation'][len(static_str_from_prompt):] + \n",
    "                    dataset['validation'][int(i)]['human_reference'] + \n",
    "                    '\\n\\n'\n",
    "                )\n",
    "            context += 'Consider the above coherent sentences from concepts. Just generate one sentence. '\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meaning_to_references = defaultdict(list)\n",
    "for entry in dataset['test']:\n",
    "    meaning_to_references[context + entry[\"meaning_representation\"]].append(entry[\"human_reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the test dataset\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"meaning_representation\"], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DictDataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer, input_size, \n",
    "                #  base_model_name, model_type\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list of dicts): A list where each element is a dictionary with features as keys.\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_size = input_size\n",
    "        # self.base_model_name = base_model_name\n",
    "        # self.model_type = model_type\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the data to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing the features and their corresponding values for the given index.\n",
    "        \"\"\"\n",
    "        example = self.data_list[idx]\n",
    "\n",
    "        \n",
    "        # # Tokenize the 'meaning_representation' on the fly\n",
    "        # tokenized = tokenize_function(example, self.tokenizer)\n",
    "        # inputs = f'<bos> {example[\"meaning_representation\"]} <eos>'\n",
    "        # inputs = f\"Question: Generate a natural language sentence from the following aspects: {example['meaning_representation']}\" + \"\\nAnswer:\"\n",
    "        # if(self.base_model_name):\n",
    "        #     if('gpt2-medium' in self.base_model_name):\n",
    "        #         prefix_str = 'Generate a restaurant description from the following attributes:\\n'\n",
    "        #         suffix_str = '\\n\\nDescription: '\n",
    "        #         # prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "        #         # suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "        #     elif('gpt2-large' in self.base_model_name):\n",
    "        #         # prefix_str = 'Question: Given the following attributes of a restaurant:\\n'\n",
    "        #         # suffix_str = ',\\nhow would you describe the restaurant based on the attributes? Do not provide explanation.\\nAnswer: '\n",
    "        #         prefix_str = 'Generate a restaurant description from the following attributes:\\n'\n",
    "        #         suffix_str = '\\n\\nDescription: '\n",
    "        #         # prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "        #         # suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "        #     elif('Llama-3.1-8B' in self.base_model_name):\n",
    "        #         prefix_str = 'Question: Given the following attributes of a restaurant:\\n'\n",
    "        #         suffix_str = ',\\nhow would you describe the restaurant based on the attributes? Do not provide explanation.\\nAnswer: '\n",
    "        #         # prefix_str = 'Generate a restaurant description from the following attributes:\\n'\n",
    "        #         # suffix_str = '\\n\\nDescription: '\n",
    "        #     else:\n",
    "        #         prefix_str = ''\n",
    "        #         suffix_str = ''\n",
    "        # else:\n",
    "        #     if(self.model_type == 'gpt2'):\n",
    "        #         prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "        #         suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "        #     elif(self.model_type == 'llama'):\n",
    "        #         prefix_str = 'Question: Given the following attributes of a restaurant, \"'\n",
    "        #         suffix_str = '\", how would you describe the restaurant based on the attributes? Just provide the description with no explanation.\\nAnswer: '\n",
    "        #     else:\n",
    "        #         prefix_str = ''\n",
    "        #         suffix_str = ''\n",
    "        # inputs = prefix_str + parse_mr_to_string(example[\"meaning_representation\"]) + suffix_str\n",
    "        inputs = f'{example[\"meaning_representation\"]}'\n",
    "        tokenized = self.tokenizer(inputs, return_tensors=\"pt\", max_length=self.input_size, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        \n",
    "        # Return the tokenized inputs along with any other features (like labels)\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'meaning_representation' :   example['meaning_representation']# Remove batch dimension\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_mr_to_string(mr):\n",
    "#     # Regular expression to capture key-value pairs\n",
    "#     pattern = r'(\\w+)\\[([^\\]]+)\\]'\n",
    "    \n",
    "#     # Find all key-value pairs in the string\n",
    "#     key_value_pairs = re.findall(pattern, mr)\n",
    "    \n",
    "#     # Create a formatted string of key-value pairs\n",
    "#     formatted_string = ',\\n'.join([f'{key}: {value}' for key, value in key_value_pairs])\n",
    "    \n",
    "#     return formatted_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DictDataset(Dataset):\n",
    "#     def __init__(self, data_list, tokenizer, input_size, base_model_name, model_type):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             data_list (list of dicts): A list where each element is a dictionary with features as keys.\n",
    "#         \"\"\"\n",
    "#         self.data_list = data_list\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.input_size = input_size\n",
    "#         self.base_model_name = base_model_name\n",
    "#         self.model_type = model_type\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Return the length of the dataset.\"\"\"\n",
    "#         return len(self.data_list)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             idx (int): Index of the data to retrieve.\n",
    "        \n",
    "#         Returns:\n",
    "#             dict: A dictionary containing the features and their corresponding values for the given index.\n",
    "#         \"\"\"\n",
    "#         example = self.data_list[idx]\n",
    "        \n",
    "#         # # Tokenize the 'meaning_representation' on the fly\n",
    "#         # tokenized = tokenize_function(example, self.tokenizer)\n",
    "#         # inputs = f'<bos> {example[\"meaning_representation\"]} <eos>'\n",
    "#         # inputs = f\"Question: Generate a natural language sentence from the following aspects: {example['meaning_representation']}\" + \"\\nAnswer:\"\n",
    "#         if(self.base_model_name):\n",
    "#             if('gpt2-medium' in self.base_model_name):\n",
    "#                 prefix_str = 'Generate a restaurant description from the following attributes:\\n'\n",
    "#                 suffix_str = '\\n\\nDescription: '\n",
    "#                 # prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "#                 # suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "#             elif('gpt2-large' in self.base_model_name):\n",
    "#                 # prefix_str = 'Question: Given the following attributes of a restaurant:\\n'\n",
    "#                 # suffix_str = ',\\nhow would you describe the restaurant based on the attributes? Do not provide explanation.\\nAnswer: '\n",
    "#                 prefix_str = 'Generate a restaurant description from the following attributes:\\n'\n",
    "#                 suffix_str = '\\n\\nDescription: '\n",
    "#                 # prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "#                 # suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "#             elif('Llama-3.1-8B' in self.base_model_name):\n",
    "#                 prefix_str = 'Question: Given the following attributes of a restaurant:\\n'\n",
    "#                 suffix_str = ',\\nhow would you describe the restaurant based on the attributes? Do not provide explanation.\\nAnswer: '\n",
    "#                 # prefix_str = 'Generate a restaurant description from the following attributes:\\n'\n",
    "#                 # suffix_str = '\\n\\nDescription: '\n",
    "#             else:\n",
    "#                 prefix_str = ''\n",
    "#                 suffix_str = ''\n",
    "#         else:\n",
    "#             if(self.model_type == 'gpt2'):\n",
    "#                 prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "#                 suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "#             elif(self.model_type == 'llama'):\n",
    "#                 prefix_str = 'Question: Given the following attributes of a restaurant, \"'\n",
    "#                 suffix_str = '\", how would you describe the restaurant based on the attributes? Just provide the description with no explanation.\\nAnswer: '\n",
    "#             else:\n",
    "#                 prefix_str = ''\n",
    "#                 suffix_str = ''\n",
    "#         inputs = prefix_str + parse_mr_to_string(example[\"meaning_representation\"]) + suffix_str\n",
    "#         # inputs = f'{example[\"meaning_representation\"]}'\n",
    "#         tokenized = self.tokenizer(inputs, return_tensors=\"pt\", max_length=self.input_size, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        \n",
    "#         # Return the tokenized inputs along with any other features (like labels)\n",
    "#         return {\n",
    "#             'input_ids': tokenized['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "#             'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
    "#             'meaning_representation' :   example['meaning_representation']# Remove batch dimension\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_dataset = DictDataset([{'meaning_representation': mr} for mr in meaning_to_references.keys()], tokenizer, \n",
    "                             INPUT_SIZE, \n",
    "                            #  BASE_PROC_NAME, MODEL_TYPE\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          3198,  6827,   326,  3544,   477,   262,  1708, 10838,    25,  2005,\n",
       "            11,  9845,    11,  9592,    11,   318,    25,   220,   198, 29753,\n",
       "          4713,  8246, 12023,   351,  9845,   287,   262,  9592,   198,   198,\n",
       "          3198,  6827,   326,  3544,   477,   262,  1708, 10838,    25,   670,\n",
       "            11,  4572,    11,  5806,    11,   318,    25,   220,   198,    40,\n",
       "          5806, 14153, 15232,   981,  1762,   319,   262,  4572,    13,   198,\n",
       "           198,  3198,  6827,   326,  3544,   477,   262,  1708, 10838,    25,\n",
       "          5806,    11, 38278,    11,  3708,    11,   318,    25,   220,   198,\n",
       "           464, 18739, 12408,   257, 15224,   981,  5059,   262, 38278,    13,\n",
       "           198,   198,  3198,  6827,   326,  3544,   477,   262,  1708, 10838,\n",
       "            25, 12797,    11,  1650,    11,  3056,    11,  4314,    11, 18757,\n",
       "            11,   318,    25,   220]),\n",
       " 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'meaning_representation': 'One sentence that uses all the following concepts: cut, knife, kitchen, is: \\ncutting fresh raw beef with knife in the kitchen\\n\\nOne sentence that uses all the following concepts: work, machine, wear, is: \\nI wear protective glasses while working on the machine.\\n\\nOne sentence that uses all the following concepts: wear, tractor, drive, is: \\nThe farmer wore a jacket while driving the tractor.\\n\\nOne sentence that uses all the following concepts: pour, sit, oil, floor, motorcycle, is: '}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load BLEU and ROUGE metrics from evaluate library\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "nist_metric = NLGMetricverse(metrics=load_metric(\"nist\"), run_concurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_position_ids(input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Generate position IDs for left-padded sequences.\n",
    "    \n",
    "    Args:\n",
    "        input_ids (torch.Tensor): Tensor of input token IDs (shape: [batch_size, seq_len]).\n",
    "        attention_mask (torch.Tensor): Tensor of attention mask (shape: [batch_size, seq_len]).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Position IDs (shape: [batch_size, seq_len]).\n",
    "    \"\"\"\n",
    "    # Get the lengths of the non-padded tokens (i.e., count of '1's in the attention mask)\n",
    "    seq_lengths = attention_mask.sum(dim=-1)\n",
    "\n",
    "    # Create a tensor with position IDs starting from 0 for each non-padded token\n",
    "    position_ids = torch.arange(input_ids.size(1), dtype=torch.long).unsqueeze(0).repeat(input_ids.size(0), 1).to(input_ids.device)\n",
    "\n",
    "    # Adjust position IDs for each sequence to start from 0 after padding\n",
    "    position_ids = position_ids - (input_ids.size(1) - seq_lengths).unsqueeze(-1)\n",
    "\n",
    "    # Set position IDs for padding tokens to 0 (optional: you can use another value if needed)\n",
    "    position_ids = torch.where(attention_mask == 1, position_ids, torch.zeros_like(position_ids))\n",
    "    return position_ids.long()\n",
    "\n",
    "# def get_position_ids(input_ids, attention_mask):\n",
    "#     position_ids = torch.arange(input_ids.size(1)).expand_as(input_ids).to(input_ids.device)\n",
    "#     position_ids = position_ids * attention_mask\n",
    "#     return position_ids.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def custom_generate(model, input_ids, attention_mask, max_length, repetition_penalty, tokenizer, top_k=50, temperature=1.0, top_p = 1, bb_model = None):\n",
    "    \n",
    "#     generated_ids = input_ids.clone().to(model.device)  # Start with the input prompt\n",
    "#     # print(generated_ids)\n",
    "#     # finished_sequences = torch.zeros(input_ids.size(0), dtype=torch.bool).to(model.device)\n",
    "        \n",
    "#     for step in range(max_length-input_ids.size()[1]):\n",
    "#         # Get the model outputs (logits) for the current step\n",
    "#         # with torch.no_grad():\n",
    "#         position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "#         # print('position ids', position_ids)\n",
    "#         outputs = model(input_ids=generated_ids, attention_mask=attention_mask, use_cache=True, position_ids=position_ids)\n",
    "#         logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "#         # print(logits.size())\n",
    "#         min_logit = torch.min(logits, dim=-1).values\n",
    "#         min_logit += torch.ones_like(min_logit, dtype=torch.float)\n",
    "#         min_logit = min_logit.reshape((1, len(min_logit)))\n",
    "#         logits += min_logit.T\n",
    "        \n",
    "# #         min_logit = torch.absolute(torch.min(logits)) + 1.0\n",
    "# #         logits += min_logit\n",
    "# #         # logits = apply_repetition_penalty(logits, generated_ids, repetition_penalty)\n",
    "        \n",
    "# #         # # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "#         for i, gen_id in enumerate(generated_ids):\n",
    "#             for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "#                 # if logits[i, token_id] > 0:\n",
    "#                 logits[i, token_id] /= repetition_penalty\n",
    "#                 # else:\n",
    "#                 #     logits[i, token_id] *= repetition_penalty\n",
    "\n",
    "#         # Apply temperature\n",
    "#         # logits = apply_temperature(logits, temperature)\n",
    "\n",
    "#         # Convert logits to probabilities\n",
    "#         probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "#         if(bb_model):\n",
    "#             # with torch.no_grad():\n",
    "#             position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "#             outputs_base = bb_model(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "#             logits_base = outputs_base.logits[:, -1, :]  # Get logits of the last token\n",
    "            \n",
    "#             min_logit = torch.min(logits_base, dim=-1).values\n",
    "#             min_logit += torch.ones_like(min_logit, dtype=torch.float)\n",
    "#             min_logit = min_logit.reshape((1, len(min_logit)))\n",
    "#             logits_base += min_logit.T\n",
    "\n",
    "#             # min_logit = torch.absolute(torch.min(logits_base)) + 1.0\n",
    "#             # logits_base += min_logit\n",
    "        \n",
    "#             # logits_base = apply_repetition_penalty(logits_base, generated_ids, repetition_penalty)\n",
    "#             # # # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "#             for i, gen_id in enumerate(generated_ids):\n",
    "#                 for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "#             #         # if logits_base[i, token_id] > 0:\n",
    "#                     logits_base[i, token_id] /= repetition_penalty\n",
    "#             #         # else:\n",
    "#             #         #     logits_base[i, token_id] *= repetition_penalty\n",
    "#             #         # logits_base[i, token_id] /= repetition_penalty\n",
    "#             probs_base = torch.nn.functional.softmax(logits_base, dim=-1)\n",
    "#             # print('middle', torch.max(probs_base, dim=-1))\n",
    "#             probs = probs*probs_base\n",
    "#         sum_probs = probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "#         # Avoid division by zero by adding a small value (epsilon)\n",
    "#         sum_probs = torch.clamp(sum_probs, min=1e-9)\n",
    "\n",
    "#             # Re-normalize by dividing each probability by the sum of probabilities\n",
    "#         probs = probs / sum_probs\n",
    "\n",
    "#         # Sample the next token using top-k sampling\n",
    "#         # next_token = sample_top_k(probs, top_k)\n",
    "#         # next_token = sample_top_k_top_p(probs, top_k, top_p)\n",
    "#         next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "#         # next_token = torch.where(finished_sequences.unsqueeze(-1), tokenizer.pad_token_id, next_token)\n",
    "\n",
    "#         # Append the new token to the generated sequence\n",
    "#         generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "\n",
    "#         # Extend the attention mask to include the newly generated token\n",
    "#         new_attention_mask = torch.ones((attention_mask.shape[0], 1)).to(model.device)\n",
    "#         attention_mask = torch.cat((attention_mask, new_attention_mask), dim=-1)\n",
    "        \n",
    "#         # finished_sequences |= next_token.squeeze(-1) == tokenizer.eos_token_id\n",
    "\n",
    "#         # If all sequences are finished, break the loop\n",
    "#         # if finished_sequences.all():\n",
    "#         #     break\n",
    "\n",
    "#         # # # Break if the model generates an end-of-sequence token\n",
    "#         if torch.all(next_token == tokenizer.eos_token_id):\n",
    "#             break\n",
    "#     return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_generate_original(model, input_ids, attention_mask, max_length, repetition_penalty, tokenizer, top_k=50, temperature=1.0, top_p = 1, bb_model = None):\n",
    "    \n",
    "    # print('model device is', model.device)\n",
    "    # print('input device is', input_ids.device)\n",
    "#     bos_token_id = tokenizer.bos_token_id\n",
    "#     bos_token = torch.full((input_ids.shape[0], 1), bos_token_id, dtype=torch.long).to(model.device)\n",
    "#     input_ids = torch.cat((input_ids, bos_token), dim=-1)  # Prepend BOS token to input_ids\n",
    "#     attention_mask = torch.cat((attention_mask, torch.ones_like(bos_token)), dim=-1)  # Update attention mask\n",
    "    \n",
    "    generated_ids = input_ids.clone()  # Start with the input prompt\n",
    "    finished_sequences = torch.zeros(input_ids.size(0), dtype=torch.bool).to(input_ids.device)\n",
    "    k = 0\n",
    "    \n",
    "    position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "    # print('$$$$$$$$$$$$$$$')\n",
    "    # print('iids', input_ids[7, :])\n",
    "    # print('ams', attention_mask[7, :])\n",
    "    # print('pids', position_ids[7, :])\n",
    "    past_key_values = None\n",
    "    past_key_values_base = None\n",
    "        \n",
    "    # for step in range(max_length-input_ids.size()[1]):\n",
    "    for step in range(max_length):\n",
    "        # Get the model outputs (logits) for the current step\n",
    "        with torch.no_grad():\n",
    "            # if(past_key_values):\n",
    "            #     outputs = model(input_ids=generated_ids[:, -1:], attention_mask=attention_mask[:, -1:], use_cache=True, position_ids=position_ids[:, -1:], past_key_values=past_key_values)\n",
    "            # else:\n",
    "            outputs = model(input_ids=generated_ids, attention_mask=attention_mask, use_cache=True, position_ids=position_ids)\n",
    "            # print('outputs.logits', outputs.logits.size())\n",
    "            logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "            # print('logits', torch.argmax(logits[7, :]))\n",
    "            # past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # min_logit = torch.min(logits, dim=-1).values\n",
    "            # min_logit += torch.ones_like(min_logit, dtype=torch.float)\n",
    "            # min_logit = min_logit.reshape((1, len(min_logit)))\n",
    "            # logits += min_logit.T\n",
    "            \n",
    "        k+=1\n",
    "        # if(k==1):\n",
    "        # print(logits.size())\n",
    "        # print(logits)\n",
    "        # print(torch.max(logits, dim=-1))\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "        # probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "        # for i, gen_id in enumerate(generated_ids[:, INPUT_SIZE:]):\n",
    "        # print('generated ids size', generated_ids.size())\n",
    "        for i, gen_id in enumerate(generated_ids):\n",
    "            # print('gen id', torch.unique(gen_id))\n",
    "            # print(f'$ step {step}')\n",
    "            for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "                # print(f'# step {step}')\n",
    "                if logits[i, token_id] > 0:\n",
    "                    logits[i, token_id] /= repetition_penalty\n",
    "                else:\n",
    "                    logits[i, token_id] *= repetition_penalty\n",
    "                # logits[i, token_id] /= repetition_penalty\n",
    "\n",
    "        # Apply temperature\n",
    "        # logits = apply_temperature(logits, temperature)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # print('before prob argmax', torch.argmax(probs[7, :]))\n",
    "        # print('before prob max', torch.max(probs[7, :]))\n",
    "        # print('base', torch.max(probs, dim=-1))\n",
    "        \n",
    "        if(bb_model):\n",
    "            with torch.no_grad():\n",
    "                # position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "                # if(past_key_values_base):\n",
    "                #     outputs_base = bb_model(input_ids=generated_ids[:, -1:], attention_mask=attention_mask[:, -1:], use_cache=True, \n",
    "                #                             position_ids=position_ids[:, -1:], past_key_values=past_key_values_base)\n",
    "                # else:\n",
    "                outputs_base = bb_model(input_ids=generated_ids, attention_mask=attention_mask, use_cache=True, position_ids=position_ids)\n",
    "                logits_base = outputs_base.logits[:, -1, :]  # Get logits of the last token\n",
    "                # print(logits_base)\n",
    "                # print('logits_base', torch.argmax(logits_base[7, :]))\n",
    "                # past_key_values_base = outputs_base.past_key_values\n",
    "                \n",
    "                # min_logit = torch.min(logits_base, dim=-1).values\n",
    "                # min_logit += torch.ones_like(min_logit, dtype=torch.float)\n",
    "                # min_logit = min_logit.reshape((1, len(min_logit)))\n",
    "                # logits_base += min_logit.T\n",
    "\n",
    "            # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "            # for i, gen_id in enumerate(generated_ids[:, INPUT_SIZE:]):\n",
    "            for i, gen_id in enumerate(generated_ids):\n",
    "                for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "                    if logits_base[i, token_id] > 0:\n",
    "                        logits_base[i, token_id] /= repetition_penalty\n",
    "                    else:\n",
    "                        logits_base[i, token_id] *= repetition_penalty\n",
    "                    # logits_base[i, token_id] /= repetition_penalty\n",
    "            probs_base = torch.nn.functional.softmax(logits_base, dim=-1)\n",
    "            # print('middle', torch.max(probs_base, dim=-1))\n",
    "            probs = probs*probs_base\n",
    "            sum_probs = probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "            # Avoid division by zero by adding a small value (epsilon)\n",
    "            sum_probs = torch.clamp(sum_probs, min=1e-9)\n",
    "\n",
    "            # Re-normalize by dividing each probability by the sum of probabilities\n",
    "            probs = probs / sum_probs\n",
    "\n",
    "        # print('after prob argmax', torch.argmax(probs[7, :]))\n",
    "        # print('after prob max', torch.max(probs[7, :]))\n",
    "        # print('final', torch.max(probs, dim=-1))\n",
    "\n",
    "        # Sample the next token using top-k sampling\n",
    "        # next_token = sample_top_k(probs, top_k)\n",
    "        # next_token = sample_top_k_top_p(probs, top_k, top_p)\n",
    "        next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        next_token = torch.where(finished_sequences.unsqueeze(-1), tokenizer.pad_token_id, next_token)\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "\n",
    "        # Extend the attention mask to include the newly generated token\n",
    "        new_attention_mask = torch.ones((attention_mask.shape[0], 1)).to(input_ids.device)\n",
    "        attention_mask = torch.cat((attention_mask, new_attention_mask), dim=-1)\n",
    "        \n",
    "        finished_sequences |= next_token.squeeze(-1) == tokenizer.eos_token_id\n",
    "        \n",
    "        last_values = position_ids[:, -1]  # This gets the last value of each row (shape: m)\n",
    "        new_values = last_values + 1  # Increment each last value by 1\n",
    "        new_values = new_values.unsqueeze(1)  # Reshape to (m, 1) to concatenate with the tensor\n",
    "        position_ids = torch.cat([position_ids, new_values], dim=1) \n",
    "        \n",
    "        if finished_sequences.all():\n",
    "            # eos_tensor = torch.full( (input_ids.size()[0], (max_length-input_ids.size()[1])-step-1), tokenizer.eos_token_id).to(input_ids.device)\n",
    "            eos_tensor = torch.full( (input_ids.size()[0], max_length-step-1), tokenizer.eos_token_id).to(input_ids.device)\n",
    "            generated_ids = torch.cat((generated_ids, eos_tensor), dim=1)\n",
    "            break\n",
    "\n",
    "        # Break if the model generates an end-of-sequence token\n",
    "        # if torch.all(next_token == tokenizer.eos_token_id):\n",
    "        #     break\n",
    "    # print('size of generated ids', generated_ids.size())\n",
    "    # print(generated_ids)\n",
    "    return generated_ids[:, INPUT_SIZE:]\n",
    "    # return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_generate(model, input_ids, attention_mask, max_length, repetition_penalty, tokenizer, top_k=50, temperature=1.0, top_p = 1, bb_model = None):\n",
    "    \n",
    "    # print('model device is', model.device)\n",
    "    print('input device is', input_ids.device)\n",
    "#     bos_token_id = tokenizer.bos_token_id\n",
    "#     bos_token = torch.full((input_ids.shape[0], 1), bos_token_id, dtype=torch.long).to(model.device)\n",
    "#     input_ids = torch.cat((input_ids, bos_token), dim=-1)  # Prepend BOS token to input_ids\n",
    "#     attention_mask = torch.cat((attention_mask, torch.ones_like(bos_token)), dim=-1)  # Update attention mask\n",
    "    \n",
    "    generated_ids = input_ids.clone()  # Start with the input prompt\n",
    "    finished_sequences = torch.zeros(input_ids.size(0), dtype=torch.bool).to(input_ids.device)\n",
    "    k = 0\n",
    "        \n",
    "    for step in range(max_length-input_ids.size()[1]):\n",
    "        # Get the model outputs (logits) for the current step\n",
    "        with torch.no_grad():\n",
    "            # print(generated_ids)\n",
    "            # print(attention_mask)\n",
    "            position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "            outputs = model(input_ids=generated_ids, attention_mask=attention_mask, use_cache=True, position_ids=position_ids)\n",
    "            # print('outputs.logits', outputs.logits.size())\n",
    "            logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "            \n",
    "            # min_logit = torch.min(logits, dim=-1).values\n",
    "            # min_logit += torch.ones_like(min_logit, dtype=torch.float)\n",
    "            # min_logit = min_logit.reshape((1, len(min_logit)))\n",
    "            # logits += min_logit.T\n",
    "            \n",
    "        k+=1\n",
    "        # if(k==1):\n",
    "        # print(logits.size())\n",
    "        # print(logits)\n",
    "        # print(torch.max(logits, dim=-1))\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "        # probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "        # for i, gen_id in enumerate(generated_ids):\n",
    "        #     for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "        #         if logits[i, token_id] > 0:\n",
    "        #             logits[i, token_id] /= repetition_penalty\n",
    "        #         else:\n",
    "        #             logits[i, token_id] *= repetition_penalty\n",
    "                # logits[i, token_id] /= repetition_penalty\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            # Split generated_ids and logits across GPUs\n",
    "            split_generated_ids = torch.split(generated_ids, generated_ids.size(0) // device_count)\n",
    "            split_logits = torch.split(logits, logits.size(0) // device_count)\n",
    "\n",
    "            # Apply the processing in parallel for each GPU\n",
    "            processed_logits = []\n",
    "\n",
    "            for gen_ids, logit in zip(split_generated_ids, split_logits):\n",
    "                # Move to the appropriate device\n",
    "                gen_ids = gen_ids.to(logit.device)\n",
    "\n",
    "                # Perform token repetition penalty logic on the GPU\n",
    "                for i, gen_id in enumerate(gen_ids):\n",
    "                    for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "                        if logit[i, token_id] > 0:\n",
    "                            logit[i, token_id] /= repetition_penalty\n",
    "                        else:\n",
    "                            logit[i, token_id] *= repetition_penalty\n",
    "                processed_logits.append(logit)\n",
    "\n",
    "            # After processing, concatenate the logits back together\n",
    "            logits = torch.cat(processed_logits, dim=0)\n",
    "\n",
    "        else:\n",
    "            # If only 1 GPU, apply the logic directly as before\n",
    "            for i, gen_id in enumerate(generated_ids):\n",
    "                for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "                    if logits[i, token_id] > 0:\n",
    "                        logits[i, token_id] /= repetition_penalty\n",
    "                    else:\n",
    "                        logits[i, token_id] *= repetition_penalty\n",
    "\n",
    "        # Apply temperature\n",
    "        # logits = apply_temperature(logits, temperature)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # print('base', torch.max(probs, dim=-1))\n",
    "        \n",
    "        if(bb_model):\n",
    "            with torch.no_grad():\n",
    "                position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "                outputs_base = bb_model(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "                logits_base = outputs_base.logits[:, -1, :]  # Get logits of the last token\n",
    "                \n",
    "                # min_logit = torch.min(logits_base, dim=-1).values\n",
    "                # min_logit += torch.ones_like(min_logit, dtype=torch.float)\n",
    "                # min_logit = min_logit.reshape((1, len(min_logit)))\n",
    "                # logits_base += min_logit.T\n",
    "\n",
    "            # Apply repetition penalty by decreasing the logits for previously generated tokens\n",
    "            for i, gen_id in enumerate(generated_ids):\n",
    "                for token_id in torch.unique(gen_id):  # Get unique tokens in the sequence\n",
    "                    if logits_base[i, token_id] > 0:\n",
    "                        logits_base[i, token_id] /= repetition_penalty\n",
    "                    else:\n",
    "                        logits_base[i, token_id] *= repetition_penalty\n",
    "                    # logits_base[i, token_id] /= repetition_penalty\n",
    "            probs_base = torch.nn.functional.softmax(logits_base, dim=-1)\n",
    "            # print('middle', torch.max(probs_base, dim=-1))\n",
    "            probs = probs*probs_base\n",
    "            sum_probs = probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "            # Avoid division by zero by adding a small value (epsilon)\n",
    "            sum_probs = torch.clamp(sum_probs, min=1e-9)\n",
    "\n",
    "            # Re-normalize by dividing each probability by the sum of probabilities\n",
    "            probs = probs / sum_probs\n",
    "            \n",
    "        # print('final', torch.max(probs, dim=-1))\n",
    "\n",
    "        # Sample the next token using top-k sampling\n",
    "        # next_token = sample_top_k(probs, top_k)\n",
    "        # next_token = sample_top_k_top_p(probs, top_k, top_p)\n",
    "        next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        # next_token = torch.where(finished_sequences.unsqueeze(-1), tokenizer.pad_token_id, next_token)\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "\n",
    "        # Extend the attention mask to include the newly generated token\n",
    "        new_attention_mask = torch.ones((attention_mask.shape[0], 1)).to(input_ids.device)\n",
    "        attention_mask = torch.cat((attention_mask, new_attention_mask), dim=-1)\n",
    "        \n",
    "        finished_sequences |= next_token.squeeze(-1) == tokenizer.eos_token_id\n",
    "        \n",
    "        if finished_sequences.all():\n",
    "            break\n",
    "\n",
    "        # Break if the model generates an end-of-sequence token\n",
    "        # if torch.all(next_token == tokenizer.eos_token_id):\n",
    "        #     break\n",
    "    # print('size of generated ids', generated_ids.size())\n",
    "    # print(generated_ids)\n",
    "    return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preds_fast(md, dat, tokenizer, repetition_penalty=1.1, max_length=128, bb_model = None, generate = 'default'):\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    mrs = []\n",
    "    \n",
    "    b = 0\n",
    "    md.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dat)):\n",
    "            input_ids = batch['input_ids']\n",
    "            # print(input_ids.size())\n",
    "            attention_mask = batch['attention_mask']\n",
    "            print('device is', input_ids.device)\n",
    "            # if(generate == 'default'):\n",
    "            #     generated_ids = md.generate(input_ids=input_ids, \n",
    "            #                                 attention_mask=attention_mask, \n",
    "            #                                 max_length=max_length, \n",
    "            #                                 pad_token_id=tokenizer.eos_token_id,\n",
    "            #                                 bos_token_id=tokenizer.bos_token_id,\n",
    "            #                                 eos_token_id=tokenizer.eos_token_id,\n",
    "            #                                 repetition_penalty=repetition_penalty, \n",
    "            #                                 do_sample = False, \n",
    "            #                                 use_cache = True,\n",
    "            #                                 length_penalty=1.0,\n",
    "            #                                 early_stopping=False,\n",
    "            #                                 num_beams=1,\n",
    "            #                                 # renormalize_logits = True, \n",
    "            #                                 output_logits = True,\n",
    "            #                                 output_scores = True,\n",
    "            #                                 return_dict_in_generate = True)\n",
    "            #     for i in range(len(generated_ids.sequences)):\n",
    "            #         generated_text = tokenizer.decode(generated_ids.sequences[i], skip_special_tokens=True)\n",
    "            #         predictions.append(generated_text)\n",
    "            # else:\n",
    "            generated_ids = custom_generate(md, \n",
    "                                            input_ids=input_ids, \n",
    "                                            attention_mask=attention_mask, \n",
    "                                            max_length=max_length, \n",
    "                                            repetition_penalty=repetition_penalty, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            bb_model = bb_model)\n",
    "                # print(input_ids.size())\n",
    "                # print(generated_ids.size())\n",
    "            generated_ids=generated_ids[:, input_ids.size()[1]:]\n",
    "                # print(generated_ids.size())\n",
    "                # print(generated_ids.size())\n",
    "#                 for i in range(len(generated_ids)):\n",
    "#                     generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "\n",
    "#             # Append generated text and reference to lists\n",
    "#                     predictions.append(generated_text)\n",
    "#             for mr in batch[\"meaning_representation\"]:\n",
    "#                 references.append(meaning_to_references[mr])\n",
    "#                 mrs.append(mr)\n",
    "#             b+=1\n",
    "#             if(b==100):\n",
    "#                 break\n",
    "#     return {'predictions': predictions, 'meaning_representation' : mrs, 'references': references}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preds(md, dat, device, tokenizer, repetition_penalty=1.1, max_length=128, bb_model = None, generate = 'default'):\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    mrs = []\n",
    "    \n",
    "    b = 0\n",
    "    md.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dat):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            # print(input_ids.size())\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            if(generate == 'default'):\n",
    "                generated_ids = md.generate(input_ids=input_ids, \n",
    "                                            attention_mask=attention_mask, \n",
    "                                            max_length=max_length, \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            repetition_penalty=repetition_penalty, \n",
    "                                            do_sample = False, \n",
    "                                            use_cache = True,\n",
    "                                            length_penalty=1.0,\n",
    "                                            early_stopping=False,\n",
    "                                            num_beams=1,\n",
    "                                            # renormalize_logits = True, \n",
    "                                            output_logits = True,\n",
    "                                            output_scores = True,\n",
    "                                            return_dict_in_generate = True)\n",
    "                for i in range(len(generated_ids.sequences)):\n",
    "                    generated_text = tokenizer.decode(generated_ids.sequences[i], skip_special_tokens=True)\n",
    "                    predictions.append(generated_text)\n",
    "            else:\n",
    "                generated_ids = custom_generate(md, \n",
    "                                                input_ids=input_ids, \n",
    "                                                attention_mask=attention_mask, \n",
    "                                                max_length=max_length, \n",
    "                                                repetition_penalty=repetition_penalty, \n",
    "                                                tokenizer=tokenizer, \n",
    "                                                bb_model = bb_model)\n",
    "                # print(input_ids.size())\n",
    "                # print(generated_ids.size())\n",
    "                generated_ids=generated_ids[:, input_ids.size()[1]:]\n",
    "                # print(generated_ids.size())\n",
    "                # print(generated_ids.size())\n",
    "                for i in range(len(generated_ids)):\n",
    "                    generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "\n",
    "            # Append generated text and reference to lists\n",
    "                    predictions.append(generated_text)\n",
    "            for mr in batch[\"meaning_representation\"]:\n",
    "                references.append(meaning_to_references[mr])\n",
    "                mrs.append(mr)\n",
    "            b+=1\n",
    "            if(b==2):\n",
    "                break\n",
    "    return {'predictions': predictions, 'meaning_representation' : mrs, 'references': references}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def setup(rank, world_size):\n",
    "#     \"\"\"Initialize the process group for DDP.\"\"\"\n",
    "#     dist.init_process_group(backend='nccl', init_method='env://', rank=rank, world_size=world_size)\n",
    "#     torch.cuda.set_device(rank)\n",
    "\n",
    "# def cleanup():\n",
    "#     \"\"\"Clean up after finishing DDP.\"\"\"\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "# def check(rank, world_size, unique_dataset, base_model, tokenizer):\n",
    "#     \"\"\"Training function run by each process.\"\"\"\n",
    "    \n",
    "#     # Set up the distributed environment.\n",
    "#     setup(rank, world_size)\n",
    "\n",
    "#     # Move the model to the correct device (each process works on its rank's GPU)\n",
    "#     device = torch.device(f'cuda:{rank}')\n",
    "#     base_model = base_model.to(device)\n",
    "\n",
    "#     # Wrap the model with DistributedDataParallel\n",
    "#     model = DDP(base_model, device_ids=[rank])\n",
    "\n",
    "#     # Use a DistributedSampler to split the data across GPUs\n",
    "#     sampler = DistributedSampler(unique_dataset, num_replicas=world_size, rank=rank)\n",
    "#     eval_dataloader = DataLoader(unique_dataset, batch_size=8, shuffle=False, sampler=sampler)\n",
    "\n",
    "#     # List to collect the generated_ids\n",
    "#     all_generated_ids = []\n",
    "\n",
    "#     # Iterate over the dataloader\n",
    "#     for batch in tqdm(eval_dataloader):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "#         # Call custom generate (same as before)\n",
    "#         generated_ids = custom_generate(\n",
    "#             model.module,           # Unwrap DDP to access the model\n",
    "#             input_ids=input_ids, \n",
    "#             attention_mask=attention_mask, \n",
    "#             max_length=128, \n",
    "#             repetition_penalty=1.1, \n",
    "#             tokenizer=tokenizer, \n",
    "#             bb_model=None\n",
    "#         )\n",
    "\n",
    "#         # Collect generated IDs\n",
    "#         all_generated_ids.append(generated_ids)\n",
    "\n",
    "#     # Clean up distributed process group\n",
    "#     cleanup()\n",
    "\n",
    "#     # Return generated IDs for this process\n",
    "#     return all_generated_ids\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main function to spawn multiple processes.\"\"\"\n",
    "#     world_size = torch.cuda.device_count()\n",
    "#     # unique_dataset = unique_dataset  # Define your dataset\n",
    "#     # base_model = base_model # Define your model\n",
    "#     # tokenizer = tokenizer  # Define your tokenizer\n",
    "\n",
    "#     # Spawn one process per GPU\n",
    "#     mp.spawn(check, args=(world_size, unique_dataset, base_model, tokenizer), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     os.environ['MASTER_ADDR'] = 'localhost'\n",
    "#     os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "#     unique_dataset = unique_dataset  # Define your dataset\n",
    "#     base_model = base_model # Define your model\n",
    "#     tokenizer = tokenizer  # Define your tokenizer\n",
    "    \n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['NCCL_DEBUG'] = 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(unique_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "base_model = base_model.to(device)\n",
    "base_model = torch.nn.DataParallel(base_model)\n",
    "# model_ft = model_ft.to(device)\n",
    "# model_ft = torch.nn.DataParallel(model_ft)\n",
    "# plugin_model_base = plugin_model_base.to(device)\n",
    "# plugin_model_base = torch.nn.DataParallel(plugin_model_base)\n",
    "# plugin_model_ft = plugin_model_ft.to(device)\n",
    "# plugin_model_ft = torch.nn.DataParallel(plugin_model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_eval_dat_per_model(tokenizer, eval_model, dat_loader, device, max_length=128, repetition_penalty=1.1, bb_model = None):\n",
    "    generated_ids_list = []\n",
    "    mrs = []\n",
    "    references = []\n",
    "    for b, batch in tqdm(enumerate(dat_loader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        if(bb_model):\n",
    "            generated_ids = custom_generate_original(eval_model.module, input_ids=input_ids, \n",
    "                                                    attention_mask=attention_mask, \n",
    "                                                    max_length=max_length, \n",
    "                                                    repetition_penalty=repetition_penalty, \n",
    "                                                    tokenizer=tokenizer, \n",
    "                                                    bb_model = bb_model.module)\n",
    "        else:\n",
    "            generated_ids = custom_generate_original(eval_model.module, input_ids=input_ids, \n",
    "                                                    attention_mask=attention_mask, \n",
    "                                                    max_length=max_length, \n",
    "                                                    repetition_penalty=repetition_penalty, \n",
    "                                                    tokenizer=tokenizer)\n",
    "            # generated_ids = eval_model.module.generate(input_ids=input_ids, \n",
    "            #                                 attention_mask=attention_mask, \n",
    "            #                                 max_length=max_length+INPUT_SIZE, \n",
    "            #                                 pad_token_id=tokenizer.eos_token_id,\n",
    "            #                                 bos_token_id=tokenizer.bos_token_id,\n",
    "            #                                 eos_token_id=tokenizer.eos_token_id,\n",
    "            #                                 repetition_penalty=repetition_penalty, \n",
    "            #                                 do_sample = False, \n",
    "            #                                 use_cache = True,\n",
    "            #                                 length_penalty=1.0,\n",
    "            #                                 early_stopping=False,\n",
    "            #                                 num_beams=1,\n",
    "            #                                 # renormalize_logits = True, \n",
    "            #                                 # output_logits = True,\n",
    "            #                                 # output_scores = True,\n",
    "            #                                 return_dict_in_generate = True)     \n",
    "        generated_ids_list.append(generated_ids)\n",
    "        mrs.extend(batch[\"meaning_representation\"])\n",
    "        if(b==0):\n",
    "            break\n",
    "    generated_ids_tensor = torch.vstack(generated_ids_list)\n",
    "    # generated_ids_tensor = generated_ids_list[0][0]\n",
    "    print('Length of generated ids', len(generated_ids_tensor[0]))       \n",
    "    predicted_text_list = tokenizer.batch_decode(generated_ids_tensor, skip_special_tokens=True)\n",
    "    for mr in mrs:\n",
    "        references.append(meaning_to_references[mr])\n",
    "    \n",
    "    return {'predictions' : predicted_text_list, 'meaning_representations' : mrs, 'references' : references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:48, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of generated ids 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_base = get_eval_dat_per_model(tokenizer, base_model, eval_dataloader, device, max_length=TARGET_SIZE, bb_model = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      " One sentence that uses all the following concepts: cut, knife, kitchen, is: \n",
      "cutting fresh raw beef with knife in the kitchen\n",
      "\n",
      "One sentence that uses all the following concepts: work, machine, wear, is: \n",
      "I wear protective glasses while working on the machine.\n",
      "\n",
      "One sentence that uses all the following concepts: wear, tractor, drive, is: \n",
      "The farmer wore a jacket while driving the tractor.\n",
      "\n",
      "One sentence that uses all the following concepts: ride, wear, hat, horse, is: \n",
      "\n",
      "\n",
      "PREDICTION\n",
      " \n",
      "The horse was wearing a hat when it was ridden.\n",
      "\n",
      "One sentence that uses all the following concepts: ride, wear, hat, horse, is: \n",
      "\n",
      "The horse was wearing a hat when it was ridden.\n",
      "\n",
      "One sentence that uses all the following concepts: ride, wear, hat,\n",
      "\n",
      "\n",
      "REFERENCES\n",
      " ['A man wears a hat while the man rides a horse.', 'I wear a hat when I ride a horse.', 'I wear my hat as I ride my horse.', 'The man wears a hat while riding on the horse.', 'A man wearing a hat riding a horse. ']\n"
     ]
    }
   ],
   "source": [
    "sel_id = 3\n",
    "print('INPUT\\n', output_base['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print('PREDICTION\\n', output_base['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print('REFERENCES\\n', output_base['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Given the following attributes of a restaurant:\n",
      "name: Blue Spice,\n",
      "eatType: coffee shop,\n",
      "rating: average,\n",
      "near: Crowne Plaza Hotel,\n",
      "how would you describe the restaurant based on the attributes? Do not provide explanation.\n",
      "Answer: 1. The name of the restaurant is \"Blue Spice\".\n",
      "2. It is a coffee shop.\n",
      "3. Its rating is average.\n",
      "4. It is near to the Crowne Plaza Hotel.\n",
      "5. It is a restaurant that serves coffee and other beverages, such as tea, hot chocolate, etc., but does not serve food.\n",
      "6. It has an average rating among its customers.\n",
      "7. It is located close to the Crowne Plaza Hotel.\n",
      "8. It is a type of eatery where people can go for a quick snack or drink without having to order a full meal.\n",
      "9. It is a place where one can get\n",
      "\n",
      "\n",
      "Question: Given the following attributes of a restaurant:\n",
      "name: Blue Spice,\n",
      "eatType: coffee shop,\n",
      "rating: average,\n",
      "near: Crowne Plaza Hotel,\n",
      "how would you describe the restaurant based on the attributes? Do not provide explanation.\n",
      "Answer: \n",
      "\n",
      "\n",
      "['Crowne Plaza Hotel has a coffee shop nearby with an average customer rating called Blue Spice.']\n"
     ]
    }
   ],
   "source": [
    "sel_id = 4\n",
    "print(output_base['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_base['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_base['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:50, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of generated ids 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_plugin_base = get_eval_dat_per_model(tokenizer, plugin_model_base, eval_dataloader, device, max_length=TARGET_SIZE, bb_model = base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Given the following attributes of a restaurant:\n",
      "name: Blue Spice,\n",
      "eatType: pub,\n",
      "rating: 5 out of 5,\n",
      "near: Crowne Plaza Hotel,\n",
      "how would you describe the restaurant based on the attributes? Do not provide explanation.\n",
      "Answer: 1. The name of the restaurant is Blue Spice.\n",
      "2. It is a pub, which means that it serves alcoholic beverages and food to customers who are seated at tables or bar stools inside the establishment. 3. Its rating is five out of five stars, indicating that it has been rated highly by previous customers. 4. It is located near Crowne Plaza Hotel, so people staying there may want to visit this place while they are in town. 5. This information can be used to create a description about the type of cuisine served at the restaurant, its atmosphere, prices, etc., based on these attributes alone without providing\n",
      "\n",
      "\n",
      "Question: Given the following attributes of a restaurant:\n",
      "name: Blue Spice,\n",
      "eatType: pub,\n",
      "rating: 5 out of 5,\n",
      "near: Crowne Plaza Hotel,\n",
      "how would you describe the restaurant based on the attributes? Do not provide explanation.\n",
      "Answer: \n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "sel_id = 7\n",
    "print(output_plugin_base['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_base['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_base['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [15:13, 11.86s/it]\n"
     ]
    }
   ],
   "source": [
    "output_ft = get_eval_dat_per_model(tokenizer, model_ft, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [22:11, 17.29s/it]\n"
     ]
    }
   ],
   "source": [
    "output_plugin_ft = get_eval_dat_per_model(tokenizer, plugin_model_ft, eval_dataloader, device, bb_model = model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_base = get_preds_fast(base_model, eval_dataloader, tokenizer, repetition_penalty=1.1, max_length=256, generate = 'custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_base = get_preds(base_model, eval_dataloader, device, tokenizer, repetition_penalty=1.1, max_length=256, generate = 'custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_plugin_base = get_preds(plugin_model_base, eval_dataloader, device, tokenizer, repetition_penalty=1.1, max_length=256, bb_model = base_model, generate = 'custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_ft = get_preds(model_ft, eval_dataloader, device, tokenizer, repetition_penalty=1.1, max_length=256, generate = 'custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_plugin_ft = get_preds(plugin_model_ft, eval_dataloader, device, tokenizer, repetition_penalty=1.1, max_length=256, bb_model = model_ft, generate = 'custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sel_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>Question: Given the following attributes of a restaurant, \"name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\", how would you describe the restaurant based on the attributes? Just provide the description with no explanation.\n",
      "Answer: 1\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "print(output_base['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_base['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_base['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>Question: Given the following attributes of a restaurant, \"name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\", how would you describe the restaurant based on the attributes? Just provide the description with no explanation.\n",
      "Answer: 1\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "print(output_plugin_base['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_base['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_base['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and is a pub that serves Blue Spice. It has an average customer rating of 5 out 3, it's located near Crowne Plaza Hotel in the city centre with good food at reasonable prices but its not kid friendly so you'll have to bring your own kids for this place if you're looking forward drinking wine or beer\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "print(output_ft['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_ft['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_ft['references'][sel_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Blue Spice is a pub that serves burgers and fries. It has an average customer rating, but it's located near Crowne Plaza Hotel in the city centre with good food for less than £20 per person which makes this place worth visiting if you're looking to eat out at night time or have some friends over who are\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "print(output_ft['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_ft['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_ft['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and is located near Crowne Plaza Hotel. It has a customer rating of 5 out the 10, it's cheap price range but serves Blue Spice with an average taste that isn't for everyone. Its not family-friendly so you can expect to get your kids there too if they're looking for something more expensive than £\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "print(output_plugin_ft['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_ft['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_ft['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  It is a pub near Crowne Plaza Hotel. Its customer rating of 5 out the 10 and it has an average price range, but its not cheap at all with prices ranging from £20-£25 for adults to more than £30 per head there are kids friendly atmosphere too which also serves English food in this\n",
      "\n",
      "\n",
      "name[Blue Spice], eatType[pub], customer rating[5 out of 5], near[Crowne Plaza Hotel]\n",
      "\n",
      "\n",
      "['The pub Blue Spice is based near Crowne Plaza Hotel and has a high customer rating of 5 out of 5.', 'The Blue Spice pub, near Crowne Plaza Hotel, has a customer rating of 5 out of 5.', 'If you want a pub rated 5 out of 5 pick Blue Spice. It is located near Crowne Plaza Hotel.']\n"
     ]
    }
   ],
   "source": [
    "print(output_plugin_ft['predictions'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_ft['meaning_representations'][sel_id])\n",
    "print('\\n')\n",
    "print(output_plugin_ft['references'][sel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.10711199018566825\n",
      "ROUGE Score: {'rouge1': 0.23905529318929822, 'rouge2': 0.1472084962376615, 'rougeL': 0.17859503597912602, 'rougeLsum': 0.2032884883245697}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_base = bleu_metric.compute(predictions=output_base['predictions'], references=output_base['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_base = rouge_metric.compute(predictions=output_base['predictions'], references=output_base['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_base['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.10438289041180976\n",
      "ROUGE Score: {'rouge1': 0.36718794215450246, 'rouge2': 0.21948851335912428, 'rougeL': 0.2733576345434364, 'rougeLsum': 0.29021199823270494}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_plugin_base = bleu_metric.compute(predictions=output_plugin_base['predictions'], references=output_plugin_base['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_plugin_base = rouge_metric.compute(predictions=output_plugin_base['predictions'], references=output_plugin_base['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_base['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.11495220997702839\n",
      "ROUGE Score: {'rouge1': np.float64(0.4423200656757982), 'rouge2': np.float64(0.20455949044420738), 'rougeL': np.float64(0.27035806765893067), 'rougeLsum': np.float64(0.2795669497283616)}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_ft = bleu_metric.compute(predictions=output_ft['predictions'], references=output_ft['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_ft = rouge_metric.compute(predictions=output_ft['predictions'], references=output_ft['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.14444679287572448\n",
      "ROUGE Score: {'rouge1': np.float64(0.45663970363818657), 'rouge2': np.float64(0.23051806194216068), 'rougeL': np.float64(0.301537506809099), 'rougeLsum': np.float64(0.3036088178051496)}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_ft = bleu_metric.compute(predictions=output_ft['predictions'], references=output_ft['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_ft = rouge_metric.compute(predictions=output_ft['predictions'], references=output_ft['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.13165112694158715\n",
      "ROUGE Score: {'rouge1': np.float64(0.45592852918224647), 'rouge2': np.float64(0.21901297848423412), 'rougeL': np.float64(0.2814866224068034), 'rougeLsum': np.float64(0.28406788275465594)}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_plugin_ft = bleu_metric.compute(predictions=output_plugin_ft['predictions'], references=output_plugin_ft['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_plugin_ft = rouge_metric.compute(predictions=output_plugin_ft['predictions'], references=output_plugin_ft['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.14450424587250552\n",
      "ROUGE Score: {'rouge1': np.float64(0.46442448683053694), 'rouge2': np.float64(0.2354501938124201), 'rougeL': np.float64(0.29982756969574764), 'rougeLsum': np.float64(0.2999552865979003)}\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU score\n",
    "bleu_score_plugin_ft = bleu_metric.compute(predictions=output_plugin_ft['predictions'], references=output_plugin_ft['references'])\n",
    "# Compute ROUGE score\n",
    "rouge_score_plugin_ft = rouge_metric.compute(predictions=output_plugin_ft['predictions'], references=output_plugin_ft['references'])\n",
    "\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {bleu_score_plugin_ft['bleu']}\")\n",
    "print(f\"ROUGE Score: {rouge_score_plugin_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2\n",
      "BLEU Score: 0.0315\n",
      "ROUGE Score rouge1: 0.2592\n",
      "ROUGE Score rouge2: 0.0761\n",
      "ROUGE Score rougeL: 0.1816\n",
      "ROUGE Score rougeLsum: 0.1901\n",
      "\n",
      "\n",
      "GPT2 with plugin using validation data\n",
      "BLEU Score: 0.0538\n",
      "ROUGE Score rouge1: 0.3475\n",
      "ROUGE Score rouge2: 0.1335\n",
      "ROUGE Score rougeL: 0.2148\n",
      "ROUGE Score rougeLsum: 0.222\n",
      "\n",
      "\n",
      "LoRA Finetuned GPT2  using training\n",
      "BLEU Score: 0.1444\n",
      "ROUGE Score rouge1: 0.4566\n",
      "ROUGE Score rouge2: 0.2305\n",
      "ROUGE Score rougeL: 0.3015\n",
      "ROUGE Score rougeLsum: 0.3036\n",
      "\n",
      "\n",
      "LoRA Finetuned GPT2  using training and then plugin using valudation data\n",
      "BLEU Score: 0.1445\n",
      "ROUGE Score rouge1: 0.4644\n",
      "ROUGE Score rouge2: 0.2355\n",
      "ROUGE Score rougeL: 0.2998\n",
      "ROUGE Score rougeLsum: 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the BLEU and ROUGE scores\n",
    "import numpy as np\n",
    "print('GPT2')\n",
    "print(f\"BLEU Score: {np.round(bleu_score_base['bleu'], 4)}\")\n",
    "for rg in rouge_score_base.keys():\n",
    "    print(f\"ROUGE Score {rg}: {np.round(rouge_score_base[rg], 4)}\")\n",
    "print('\\n')\n",
    "\n",
    "print('GPT2 with plugin using validation data')\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {np.round(bleu_score_plugin_base['bleu'], 4)}\")\n",
    "for rg in rouge_score_base.keys():\n",
    "    print(f\"ROUGE Score {rg}: {np.round(rouge_score_plugin_base[rg], 4)}\")\n",
    "print('\\n')\n",
    "\n",
    "print('LoRA Finetuned GPT2  using training')\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {np.round(bleu_score_ft['bleu'], 4)}\")\n",
    "for rg in rouge_score_base.keys():\n",
    "    print(f\"ROUGE Score {rg}: {np.round(rouge_score_ft[rg], 4)}\")\n",
    "print('\\n')\n",
    "\n",
    "print('LoRA Finetuned GPT2  using training and then plugin using valudation data')\n",
    "# Display the BLEU and ROUGE scores\n",
    "print(f\"BLEU Score: {np.round(bleu_score_plugin_ft['bleu'], 4)}\")\n",
    "for rg in rouge_score_base.keys():\n",
    "    print(f\"ROUGE Score {rg}: {np.round(rouge_score_plugin_ft[rg], 4)}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (statml)",
   "language": "python",
   "name": "statml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
