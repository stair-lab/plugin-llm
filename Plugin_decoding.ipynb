{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, \n",
    "                          EarlyStoppingCallback, get_linear_schedule_with_warmup, AdamW, DataCollatorForSeq2Seq)\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, LlamaConfig, LlamaForCausalLM\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "# os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "# os.environ['ddp_backend'] = 'gloo'\n",
    "# os.environ['NCCL_SOCKET_IFNAME'] = '^lo,docker,virbr,vmnet,vboxnet,wl,ww,ppp'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_c\n",
      "b/c\n"
     ]
    }
   ],
   "source": [
    "a = 'b/c'\n",
    "d = a.replace('/', '_')\n",
    "print(d)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"./configs/plugin_config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama': {'name': None,\n",
       "  'num_hidden_layers': 1,\n",
       "  'vocab_size': 128256,\n",
       "  'hidden_size': 512,\n",
       "  'num_attention_heads': 4,\n",
       "  'intermediate_size': 2048},\n",
       " 'gpt2': {'name': None, 'n_layer': 1}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['plugin_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'llama' # gpt2\n",
    "if(model_type == 'llama'):\n",
    "    access_token = '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'\n",
    "    base_model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "    # base_model_name = 'meta-llama/Llama-2-7b'\n",
    "\n",
    "else:\n",
    "    access_token = None\n",
    "    base_model_name = 'gpt2-medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROC_NAME = f'plugin_over_base_{model_type}'\n",
    "FT_MODEL_NAME = 'ft_model_no_token_without_lora_train_hyperval_concatenated'\n",
    "DATASET = 'e2e_nlg_cleaned'\n",
    "DATASET_VERSION = None # 'webnlg_challenge_2017' # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 64\n",
    "TARGET_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the E2E NLG dataset from Hugging Face datasets library\n",
    "if(DATASET_VERSION):\n",
    "    dataset = load_dataset(DATASET, DATASET_VERSION)\n",
    "else:\n",
    "    dataset = load_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_web_nlg(dataset, train_categories = ['Airport', 'Building', 'University', 'Monument', 'MeanOfTransportation'], \n",
    "                    test_categories = ['Artist', 'Politician', 'Athlete', 'ComicsCharacter', 'Astronaut', 'SportsTeam' ]):\n",
    "        # Step 1: Filter the 'train', 'dev', and 'test' datasets by category\n",
    "    def filter_by_category(example, categories):\n",
    "        return example['category'] in categories\n",
    "\n",
    "    # Filter the 'train' set to only include the \"Airport\" category\n",
    "    train_dataset = dataset['train'].filter(lambda x: filter_by_category(x, train_categories))\n",
    "\n",
    "    # Filter 'dev' and 'test' sets to only include the \"Food\" category\n",
    "    dev_dataset = dataset['dev'].filter(lambda x: filter_by_category(x, test_categories))\n",
    "    test_dataset = dataset['test'].filter(lambda x: filter_by_category(x, test_categories))\n",
    "\n",
    "    combined_dev_test = concatenate_datasets([dev_dataset, test_dataset])\n",
    "    combined_dev_test = combined_dev_test.train_test_split(test_size=0.5, seed=42)\n",
    "    # Rename the splits for clarity\n",
    "    combined_dev_test = DatasetDict({\n",
    "        'validation': combined_dev_test['train'],  # Rename 'train' split as 'validation'\n",
    "        'test': combined_dev_test['test']  # Keep 'test' split as 'test'\n",
    "    })\n",
    "\n",
    "    # Step 3: Select only one reference sentence from 'text' field based on 'comment'\n",
    "    def select_good_comment(example):\n",
    "        for i, comment in enumerate(example['lex']['comment']):\n",
    "            if comment == 'good':\n",
    "                return {'human_reference': example['lex']['text'][i]}  # Pick the sentence marked 'good'\n",
    "        return {'human_reference': ''}  # Default to the first sentence if none are marked 'good'\n",
    "\n",
    "    # Apply the function to each dataset\n",
    "    train_dataset = train_dataset.map(select_good_comment)\n",
    "    combined_dev_test['validation'] = combined_dev_test['validation'].map(select_good_comment)\n",
    "    combined_dev_test['test'] = combined_dev_test['test'].map(select_good_comment)\n",
    "\n",
    "    # Step 4: Join 'mtriple_set' list into a string separated by ';'\n",
    "    def join_mtriple_set(example):\n",
    "        return {'meaning_representation': ' ; '.join(example['modified_triple_sets']['mtriple_set'][0])}\n",
    "\n",
    "    # Apply the function to each dataset\n",
    "    train_dataset = train_dataset.map(join_mtriple_set)\n",
    "    combined_dev_test['validation'] = combined_dev_test['validation'].map(join_mtriple_set)\n",
    "    combined_dev_test['test'] = combined_dev_test['test'].map(join_mtriple_set)\n",
    "\n",
    "    # Step 5: Retain only 'meaning_representation' and 'human_reference' fields\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in ['meaning_representation', 'human_reference']])\n",
    "    combined_dev_test['validation'] = combined_dev_test['validation'].remove_columns(\n",
    "        [col for col in combined_dev_test['validation'].column_names if col not in ['meaning_representation', 'human_reference']])\n",
    "    combined_dev_test['test'] = combined_dev_test['test'].remove_columns(\n",
    "        [col for col in combined_dev_test['test'].column_names if col not in ['meaning_representation', 'human_reference']])\n",
    "\n",
    "    # Step 6 removing empty human reference\n",
    "    # Define a function to filter out rows where 'human_reference' is empty or None\n",
    "    def filter_empty_human_reference(example):\n",
    "        return example['human_reference'] is not None and example['human_reference'].strip() != ''\n",
    "\n",
    "    def filter_empty_meaning_representation(example):\n",
    "        return example['meaning_representation'] is not None and example['meaning_representation'].strip() != ''\n",
    "\n",
    "    train_dataset = train_dataset.filter(filter_empty_human_reference)\n",
    "    combined_dev_test['validation'] = combined_dev_test['validation'].filter(filter_empty_human_reference)\n",
    "    combined_dev_test['test'] = combined_dev_test['test'].filter(filter_empty_human_reference)\n",
    "\n",
    "    train_dataset = train_dataset.filter(filter_empty_meaning_representation)\n",
    "    combined_dev_test['validation'] = combined_dev_test['validation'].filter(filter_empty_meaning_representation)\n",
    "    combined_dev_test['test'] = combined_dev_test['test'].filter(filter_empty_meaning_representation)\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': combined_dev_test['validation'],\n",
    "    'test': combined_dev_test['test']\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "def process_e2e_nlg_cleaned(dataset):\n",
    "    return dataset\n",
    "\n",
    "def process_common_gen(dataset):\n",
    "    \n",
    "    # Define a function to filter rows\n",
    "    def filter_target_and_concepts(example):\n",
    "        # Check if 'man' exists as a whole word in 'target'\n",
    "        target_contains_man = bool(re.search(r'\\bman\\b', example['target']))\n",
    "\n",
    "        # Ensure 'man' and 'woman' are not in 'concepts'\n",
    "        concepts_does_not_contain_man_or_woman = all(c not in ['man', 'woman'] for c in example['concepts'])\n",
    "\n",
    "        # Return True if the conditions are met\n",
    "        return target_contains_man and concepts_does_not_contain_man_or_woman\n",
    "    \n",
    "    dataset['train'] = dataset['train'].filter(filter_target_and_concepts)\n",
    "\n",
    "    def join_concepts(example):\n",
    "        return {'meaning_representation': ' ; '.join(example['concepts'])}\n",
    "\n",
    "    # Apply the function to each dataset\n",
    "    dataset['train'] = dataset['train'].map(join_concepts)\n",
    "    dataset['validation'] = dataset['validation'].map(join_concepts)\n",
    "    dataset['test'] = dataset['test'].map(join_concepts)\n",
    "    \n",
    "    def ret_human_reference(example):\n",
    "        return {'human_reference': example['target']}\n",
    "    \n",
    "    dataset['train'] = dataset['train'].map(ret_human_reference)\n",
    "    dataset['validation'] = dataset['validation'].map(ret_human_reference)\n",
    "    dataset['test'] = dataset['test'].map(ret_human_reference)\n",
    "    \n",
    "    dataset['train'] = dataset['train'].remove_columns(\n",
    "        [col for col in dataset['train'].column_names if col not in ['meaning_representation', 'human_reference']])\n",
    "    dataset['validation'] = dataset['validation'].remove_columns(\n",
    "        [col for col in dataset['validation'].column_names if col not in ['meaning_representation', 'human_reference']])\n",
    "    dataset['test'] = dataset['test'].remove_columns(\n",
    "        [col for col in dataset['test'].column_names if col not in ['meaning_representation', 'human_reference']])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(DATASET == 'e2e_nlg_cleaned'):\n",
    "    dataset = process_e2e_nlg_cleaned(dataset)\n",
    "elif(DATASET == 'web_nlg'):\n",
    "    dataset = process_web_nlg(dataset)\n",
    "elif(DATASET == 'common_gen'):\n",
    "    dataset = process_common_gen(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_input_prompt(examples):\n",
    "    inp = examples['meaning_representation']\n",
    "    # for gpt2\n",
    "    if(model_type == 'gpt2'):\n",
    "        prefix_str = 'Given the following aspects of a restaurant, \"'\n",
    "        suffix_str = '\", a natural language sentence describing the restuarant is: '\n",
    "    # for llama\n",
    "    elif(model_type == 'llama'):\n",
    "        prefix_str = 'Question: Given the following attributes of a restaurant, \"'\n",
    "        suffix_str = '\", how would you describe the restaurant based on the attributes? Just provide the description with no explanation.\\nAnswer: '\n",
    "    new_input = prefix_str + inp + suffix_str\n",
    "    return {'meaning_representation': new_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(DATASET == 'e2e_nlg_cleaned'):\n",
    "    dataset['train'] = dataset['train'].map(add_input_prompt)\n",
    "    dataset['validation'] = dataset['validation'].map(add_input_prompt)\n",
    "    dataset['test'] = dataset['test'].map(add_input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['meaning_representation', 'human_reference'],\n",
       "    num_rows: 33525\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Llama-3.1-8B'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(f'./models/{FT_MODEL_NAME}')\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token = access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "# CHANGE THE PADDIGN TO LEFT FOR PREVIOUS BATCH FORM OF RESULTS\n",
    "tokenizer.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128256, 128000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "# base_model_name = 'gpt2-medium'\n",
    "# base_model_name = llama_model_id\n",
    "if(model_type == 'gpt2'):\n",
    "    config = GPT2Config(\n",
    "            n_layer=1,         # Fewer transformer layers (default 12)\n",
    "        )\n",
    "    base_model = GPT2LMHeadModel(config=config)\n",
    "else:\n",
    "    # config = LlamaConfig(\n",
    "    #         num_hidden_layers=1,         # Fewer transformer layers (default 12)\n",
    "    #         vocab_size=len(tokenizer.vocab),\n",
    "    #     )\n",
    "    # base_model = LlamaForCausalLM(config=config)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, token = access_token, torch_dtype=torch.float16, \n",
    "                                                    #   device_map=\"auto\", \n",
    "                                                      )\n",
    "    base_model.to('cuda:0')\n",
    "# base_model.resize_token_embeddings(len(tokenizer))\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|> <|end_of_text|> <|begin_of_text|>\n",
      "128001 128001 128000\n",
      "128001 None 128000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token, tokenizer.pad_token, tokenizer.bos_token)\n",
    "print(tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.bos_token_id)\n",
    "print(base_model.config.eos_token_id, base_model.config.pad_token_id, base_model.config.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_ft = AutoModelForCausalLM.from_pretrained(\"../../models/peft_gpt2-medium_e2e\")  # Path to your fine-tuned model\n",
    "# Create the GPT-2 model with the smaller configuration\n",
    "if('plugin_over_ft' in PROC_NAME):\n",
    "# if(PROC_NAME == 'ft_model_pad_token'):\n",
    "    # model_ft = PeftModel.from_pretrained(base_model, f'./models/{FT_MODEL_NAME}')\n",
    "    model_ft = AutoModelForCausalLM.from_pretrained(f'./models/{FT_MODEL_NAME}')\n",
    "elif('plugin_over_base' in PROC_NAME):\n",
    "    model_ft = base_model\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "# model_ft = AutoModelForCausalLM.from_pretrained('./models/ft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_ft.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the dataset to include the meaning representation (MR) as input and human reference as target\n",
    "# def preprocess_function(examples):\n",
    "#     # Concatenate MR and human reference with a separator\n",
    "#     inputs = [f\"{mr}\" for mr in examples[\"meaning_representation\"]]\n",
    "#     targets = [f\"{ref}\" for ref in examples[\"human_reference\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=INPUT_SIZE, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "#     labels = tokenizer(targets, max_length=TARGET_SIZE, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    \n",
    "#     # Replace padding token id's of the labels by -100 so that it's ignored by the loss\n",
    "#     labels[\"input_ids\"] = [\n",
    "#         [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq] \n",
    "#         for labels_seq in labels[\"input_ids\"]\n",
    "#     ]\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset to include the meaning representation (MR) as input and human reference as target\n",
    "def preprocess_function(examples):\n",
    "    # Left-pad the meaning_representation and right-pad the human reference\n",
    "    inputs = [f\"{mr}\" for mr in examples[\"meaning_representation\"]]\n",
    "    targets = [f\"{ref}\" for ref in examples[\"human_reference\"]]\n",
    "    \n",
    "    # Right-pad the human references (default padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=TARGET_SIZE, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Left-pad the meaning_representation manually\n",
    "    # We manually pad to the left by prepending padding tokens\n",
    "    max_input_length = INPUT_SIZE\n",
    "    tokenized_inputs = [tokenizer(mr, truncation=True)[\"input_ids\"] for mr in inputs]\n",
    "    left_padded_inputs = [\n",
    "        [tokenizer.pad_token_id] * (max_input_length - len(input_seq)) + input_seq if len(input_seq) < max_input_length else input_seq[:max_input_length] \n",
    "        for input_seq in tokenized_inputs\n",
    "    ]\n",
    "    \n",
    "    # Convert left-padded inputs to tensors and include attention mask\n",
    "    model_inputs = {\n",
    "        \"input_ids\": torch.tensor(left_padded_inputs),\n",
    "        \"attention_mask\": torch.tensor([[0] * (max_input_length - len(input_seq)) + [1] * len(input_seq) for input_seq in tokenized_inputs])\n",
    "    }\n",
    "\n",
    "    # Replace padding token id's of the labels by -100 so that it's ignored by the loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq] \n",
    "        for labels_seq in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    # Include the right-padded labels in the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function_batch(examples):\n",
    "    # Extract the meaning representations (MR) and human references (target text) from the examples\n",
    "    inputs = examples[\"meaning_representation\"]\n",
    "    targets = examples[\"human_reference\"]\n",
    "    \n",
    "    # Tokenize the inputs (meaning representations)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=INPUT_SIZE, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        # return_tensors=\"pt\"  # Use numpy for batch processing\n",
    "    )\n",
    "    \n",
    "    # Tokenize the targets (human references)\n",
    "    tokenized_targets = tokenizer(\n",
    "        targets, \n",
    "        max_length=TARGET_SIZE, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        # return_tensors=\"pt\"  # Use numpy for batch processing\n",
    "    )\n",
    "    \n",
    "    # Concatenate input_ids (MR) and input_ids from the targets (human reference) into one sequence\n",
    "    # This creates the full sequence: [MR, target] (all tokenized)\n",
    "    concatenated_input_ids = [\n",
    "        list(input_seq) + list(target_seq) for input_seq, target_seq in zip(tokenized_inputs[\"input_ids\"], tokenized_targets[\"input_ids\"])\n",
    "    ]\n",
    "    \n",
    "    # Concatenate attention masks for both MR and target\n",
    "    concatenated_attention_mask = [\n",
    "        list(input_mask) + list(target_mask) for input_mask, target_mask in zip(tokenized_inputs[\"attention_mask\"], tokenized_targets[\"attention_mask\"])\n",
    "    ]\n",
    "    \n",
    "    # Prepare the labels for loss computation:\n",
    "    # We need to ignore the loss for the part corresponding to MR and only compute it for the target (human reference).\n",
    "    \n",
    "    labels = []\n",
    "    for input_len, target_seq in zip([INPUT_SIZE] * len(inputs), tokenized_targets[\"input_ids\"]):\n",
    "        # Ignore loss for MR part by setting it to -100\n",
    "        labels_seq = [-100] * input_len\n",
    "        \n",
    "        # For the target sequence, we keep the tokens, but set padding tokens to -100\n",
    "        labels_seq += [token if token != tokenizer.pad_token_id else -100 for token in target_seq]\n",
    "        \n",
    "        labels.append(labels_seq)\n",
    "    \n",
    "    # Return the final dictionary containing input_ids, attention_mask, and labels\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(concatenated_input_ids),\n",
    "        \"attention_mask\": torch.tensor(concatenated_attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function_preconcat(examples):\n",
    "    # Extract the meaning representations (MR) and human references (target text) from the examples\n",
    "    inputs = examples[\"meaning_representation\"]\n",
    "    targets = examples[\"human_reference\"]\n",
    "    \n",
    "    sentences = []\n",
    "    for inp, tar in zip(inputs, targets):\n",
    "        sentences.append(inp + tar)\n",
    "    \n",
    "    # Tokenize the targets (human references)\n",
    "    tokenized_targets = tokenizer(\n",
    "        targets, \n",
    "        # add_special_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # Tokenize the inputs (meaning representations)\n",
    "    tokenized_sentences = tokenizer(\n",
    "        sentences, \n",
    "        max_length=INPUT_SIZE + TARGET_SIZE, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        # add_special_tokens=False,\n",
    "        # return_tensors=\"pt\"  # Use numpy for batch processing\n",
    "    )\n",
    "        \n",
    "    labels = []\n",
    "    for comb_seq, target_seq in zip(tokenized_sentences['input_ids'], tokenized_targets['input_ids']):\n",
    "        label_seq = [-100]*len(comb_seq)\n",
    "        label_seq[-len(target_seq):] = target_seq\n",
    "        labels.append(label_seq)\n",
    "        \n",
    "    \n",
    "    # Return the final dictionary containing input_ids, attention_mask, and labels\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(tokenized_sentences['input_ids']),\n",
    "        \"attention_mask\": torch.tensor(tokenized_sentences['attention_mask']),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Tokenize the dataset\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"meaning_representation\"], text_target=examples[\"human_reference\"], padding=\"max_length\", truncation=True, max_length=TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply tokenization to the dataset\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = dataset.map(preprocess_function_preconcat, batched=True, \n",
    "                                remove_columns=[\"meaning_representation\", \"human_reference\"]\n",
    "                               )\n",
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True, \n",
    "#                                 # remove_columns=[\"meaning_representation\", \"human_reference\"]\n",
    "#                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_dataset['validation'], tokenized_dataset['hypervalidation'] = tokenized_dataset['validation'].train_test_split(\n",
    "#     test_size=0.2, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the tokenized dataset to a pandas DataFrame for easier manipulation\n",
    "# df = pd.DataFrame(tokenized_dataset)\n",
    "\n",
    "# # Step 1: Identify unique MRs and group by MR\n",
    "# grouped_by_mr = df.groupby('meaning_representation')\n",
    "\n",
    "# # Step 2: Extract all unique MRs\n",
    "# unique_mrs = df['meaning_representation'].unique()\n",
    "\n",
    "# # Step 3: Perform train-test split on the unique MRs\n",
    "# train_mrs, hypervalidation_mrs = train_test_split(unique_mrs, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Step 4: Create new DataFrames for train and hypervalidation based on the split MRs\n",
    "# train_df = df[df['meaning_representation'].isin(train_mrs)]\n",
    "# hypervalidation_df = df[df['meaning_representation'].isin(hypervalidation_mrs)]\n",
    "\n",
    "# # Step 5: Convert back to the Dataset format for Hugging Face\n",
    "# train_dataset = DatasetDict({\"train\": Dataset.from_pandas(train_df)})\n",
    "# hypervalidation_dataset = DatasetDict({\"hypervalidation\": Dataset.from_pandas(hypervalidation_df)})\n",
    "\n",
    "# tokenized_dataset = {}\n",
    "# # Update tokenized_dataset with the new split\n",
    "# tokenized_dataset['train'] = train_dataset['train'].remove_columns([\"meaning_representation\", \"human_reference\", \"__index_level_0__\"])\n",
    "\n",
    "# tokenized_dataset['hypervalidation'] = hypervalidation_dataset['hypervalidation'].remove_columns([\"meaning_representation\", \"human_reference\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_position_ids(input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Generate position IDs for left-padded sequences.\n",
    "    \n",
    "    Args:\n",
    "        input_ids (torch.Tensor): Tensor of input token IDs (shape: [batch_size, seq_len]).\n",
    "        attention_mask (torch.Tensor): Tensor of attention mask (shape: [batch_size, seq_len]).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Position IDs (shape: [batch_size, seq_len]).\n",
    "    \"\"\"\n",
    "    # Get the lengths of the non-padded tokens (i.e., count of '1's in the attention mask)\n",
    "    seq_lengths = attention_mask.sum(dim=-1)\n",
    "\n",
    "    # Create a tensor with position IDs starting from 0 for each non-padded token\n",
    "    position_ids = torch.arange(input_ids.size(1), dtype=torch.long).unsqueeze(0).repeat(input_ids.size(0), 1).to(input_ids.device)\n",
    "\n",
    "    # Adjust position IDs for each sequence to start from 0 after padding\n",
    "    position_ids = position_ids - (input_ids.size(1) - seq_lengths).unsqueeze(-1)\n",
    "\n",
    "    # Set position IDs for padding tokens to 0 (optional: you can use another value if needed)\n",
    "    position_ids = torch.where(attention_mask == 1, position_ids, torch.zeros_like(position_ids))\n",
    "    return position_ids.long()\n",
    "\n",
    "# def get_position_ids(input_ids, attention_mask):\n",
    "#     position_ids = torch.arange(input_ids.size(1)).expand_as(input_ids).to(input_ids.device)\n",
    "#     position_ids = position_ids * attention_mask\n",
    "#     return position_ids.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_position_ids_batch(input_ids, attention_mask, split_point=64):\n",
    "    \"\"\"\n",
    "    Generate position IDs for left-padded sequences, where input_ids contains both the \n",
    "    meaning representation (MR) and human reference (target), concatenated together.\n",
    "    \n",
    "    Args:\n",
    "        input_ids (torch.Tensor): Tensor of input token IDs (shape: [batch_size, seq_len]).\n",
    "        attention_mask (torch.Tensor): Tensor of attention mask (shape: [batch_size, seq_len]).\n",
    "        split_point (int): The index at which the meaning representation ends and the human reference begins.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Position IDs (shape: [batch_size, seq_len]).\n",
    "    \"\"\"\n",
    "    # Step 1: Process the meaning representation (MR) part of the input_ids\n",
    "    # Create a tensor with position IDs starting from 0 for each non-padded token in the MR\n",
    "    mr_attention_mask = attention_mask[:, :split_point]\n",
    "    mr_position_ids = torch.arange(split_point, dtype=torch.long).unsqueeze(0).repeat(input_ids.size(0), 1).to(input_ids.device)\n",
    "    \n",
    "    # Get the lengths of the non-padded tokens in the MR\n",
    "    mr_seq_lengths = mr_attention_mask.sum(dim=-1)\n",
    "\n",
    "    # Adjust position IDs for MR to start from 0 after padding\n",
    "    mr_position_ids = mr_position_ids - (split_point - mr_seq_lengths).unsqueeze(-1)\n",
    "    \n",
    "    # Set position IDs for padding tokens in MR to 0\n",
    "    mr_position_ids = torch.where(mr_attention_mask == 1, mr_position_ids, torch.zeros_like(mr_position_ids))\n",
    "\n",
    "    # Step 2: Process the human reference (target) part of the input_ids\n",
    "    target_attention_mask = attention_mask[:, split_point:]\n",
    "    target_seq_len = input_ids.size(1) - split_point\n",
    "    target_position_ids = torch.arange(target_seq_len, dtype=torch.long).unsqueeze(0).repeat(input_ids.size(0), 1).to(input_ids.device)\n",
    "    \n",
    "    # Get the lengths of the non-padded tokens in the human reference\n",
    "    target_seq_lengths = target_attention_mask.sum(dim=-1)\n",
    "\n",
    "    # Adjust position IDs for human reference to start from 0 after padding\n",
    "    target_position_ids = target_position_ids - (target_seq_len - target_seq_lengths).unsqueeze(-1)\n",
    "    \n",
    "    # print(target_position_ids.size())\n",
    "    # print(mr_seq_lengths.size())\n",
    "    # print(target_position_ids)\n",
    "    # print(mr_seq_lengths)\n",
    "    target_position_ids += mr_seq_lengths.unsqueeze(1).repeat(1, target_position_ids.size()[1])\n",
    "    \n",
    "    # Set position IDs for padding tokens in human reference to 0\n",
    "    target_position_ids = torch.where(target_attention_mask == 1, target_position_ids, torch.zeros_like(target_position_ids))\n",
    "\n",
    "    # Step 3: Concatenate the position IDs for MR and human reference\n",
    "    position_ids = torch.cat([mr_position_ids, target_position_ids], dim=-1)\n",
    "    \n",
    "    return position_ids.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
      "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128000,  14924,\n",
      "             25,  16644,    279,   2768,   8365,    315,    264,  10960,     11,\n",
      "            330,    609,     58,    791,  36895,   1145,   8343,    941,     58,\n",
      "          79217,   8221,   1145,   3691,     58,  52566,   1145,   3430,   6174,\n",
      "             58,   1752,   1109,   7083,    508,   1145,   6130,  10959,     58,\n",
      "          10516,   1145,   3158,  16523,   1986,    579,   1145,   3070,  97605,\n",
      "             58,   9891,   1145,   3221,  33722,  35398,   6342,  19618,   1268,\n",
      "           1053,    499,   7664,    279,  10960,   3196,    389,    279,   8365,\n",
      "             30,   4702,   3493,    279,   4096,    449,    912,  16540,    627,\n",
      "          16533,  75145,  36895,    374,    264,   3428,  22359,  11033,   8221,\n",
      "           3221,  52871,   6342,    323,    279,  36617,    579,    430,    374,\n",
      "           3070,  11919,    323,    374,   2753,   1109,   7083,    508,    369,\n",
      "          11002,   3691,     13]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor(104)\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "          10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,\n",
      "          24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,\n",
      "          38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "          66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
      "          80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
      "          94,  95,  96,  97,  98,  99, 100, 101, 102, 103]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenized_dataset['train'][0]['input_ids']).reshape(1, -1)\n",
    "attention_mask = torch.tensor(tokenized_dataset['train'][0]['attention_mask']).reshape(1, -1)\n",
    "position_ids = get_position_ids(input_ids, attention_mask)\n",
    "# position_ids = position_ids * attention_mask\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "print(sum(attention_mask[0]))\n",
    "print(position_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomGPT2ModelBatch(GPT2LMHeadModel):\n",
    "    def __init__(self, config, ft_model):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.ft_model = ft_model\n",
    "\n",
    "        # for param in self.base_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get the logits from the base GPT-2 model\n",
    "        position_ids = get_position_ids(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # print(input_ids)\n",
    "        # print(attention_mask)\n",
    "        # print(position_ids)\n",
    "        # print(labels)\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        # outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        # print('logits', logits.size())\n",
    "        # Convert logits to probabilities (apply softmax)\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        # print('probabilities', probabilities.size())\n",
    "        \n",
    "        # print('no error here')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "            # outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask)\n",
    "            # print(type(outputs_base))\n",
    "            logits_base = outputs_base.logits\n",
    "            # Convert logits to probabilities (apply softmax)\n",
    "            probabilities_base = nn.functional.softmax(logits_base, dim=-1)\n",
    "        \n",
    "        # print('no error here as well')\n",
    "        \n",
    "        # Multiply the probabilities by the custom vector\n",
    "        # Ensure that the custom vector shape matches the logits shape [batch_size, seq_len, vocab_size]\n",
    "        # For this example, assume the vector applies element-wise across the vocabulary dimension\n",
    "        weighted_probabilities = probabilities * probabilities_base\n",
    "        # weighted_probabilities = probabilities\n",
    "        \n",
    "        # Normalize the probabilities again to ensure they sum to 1\n",
    "        normalized_probabilities = weighted_probabilities / weighted_probabilities.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Convert back to logits (unnormalized scores) by applying log after multiplying probabilities\n",
    "        modified_logits = torch.log(normalized_probabilities + 1e-8)  # Add small constant to avoid log(0)\n",
    "        \n",
    "        # Use modified logits for loss calculation or generation\n",
    "        if labels is not None:\n",
    "            # print('reached here')\n",
    "            # print('labels size', labels.size())\n",
    "            # print('labels', labels)\n",
    "            # Shift the logits and labels for computing the loss\n",
    "            shift_logits = modified_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # print('shift_logits', shift_logits.size())\n",
    "            # print('shift_labels', shift_labels.size())            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, modified_logits\n",
    "        \n",
    "        return modified_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT2SmallBatch(GPT2LMHeadModel):\n",
    "    def __init__(self, config, ft_model):\n",
    "        super().__init__(config)\n",
    "        self.ft_model = ft_model\n",
    "        for param in self.ft_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # for name, param in self.base_model.named_parameters():\n",
    "        #     if 'wte' not in name:\n",
    "        #         param.requires_grad = False  # Freeze all layers except lm_head\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get the logits from the base GPT-2 model\n",
    "        position_ids = get_position_ids(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # print(input_ids)\n",
    "        # print(attention_mask)\n",
    "        # print(position_ids)\n",
    "        # print(labels)\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        # outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        # print('logits', logits.size())\n",
    "        # Convert logits to probabilities (apply softmax)\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        # print('probabilities', probabilities.size())\n",
    "        \n",
    "        # print('no error here')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "            # outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask)\n",
    "            # print(type(outputs_base))\n",
    "            logits_base = outputs_base.logits\n",
    "            # Convert logits to probabilities (apply softmax)\n",
    "            probabilities_base = nn.functional.softmax(logits_base, dim=-1)\n",
    "        \n",
    "        # print('no error here as well')\n",
    "        \n",
    "        # Multiply the probabilities by the custom vector\n",
    "        # Ensure that the custom vector shape matches the logits shape [batch_size, seq_len, vocab_size]\n",
    "        # For this example, assume the vector applies element-wise across the vocabulary dimension\n",
    "        weighted_probabilities = probabilities * probabilities_base\n",
    "        # weighted_probabilities = probabilities\n",
    "        \n",
    "        # Normalize the probabilities again to ensure they sum to 1\n",
    "        normalized_probabilities = weighted_probabilities / weighted_probabilities.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Convert back to logits (unnormalized scores) by applying log after multiplying probabilities\n",
    "        modified_logits = torch.log(normalized_probabilities + 1e-8)  # Add small constant to avoid log(0)\n",
    "        \n",
    "        # Use modified logits for loss calculation or generation\n",
    "        if labels is not None:\n",
    "            # print('reached here')\n",
    "            # print('labels size', labels.size())\n",
    "            # print('labels', labels)\n",
    "            # Shift the logits and labels for computing the loss\n",
    "            shift_logits = modified_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # print('shift_logits', shift_logits.size())\n",
    "            # print('shift_labels', shift_labels.size())            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, modified_logits\n",
    "        \n",
    "        return modified_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    def __init__(self, config, ft_model):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.ft_model = ft_model\n",
    "\n",
    "        # for param in self.base_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \n",
    "        \n",
    "        generated_ids = input_ids.clone()  # Start with the input prompt\n",
    "        # finished_sequences = torch.zeros(input_ids.size(0), dtype=torch.bool).to(input_ids.device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "#         # print('position_ids', position_ids.size())\n",
    "#         # print(position_ids)\n",
    "#         outputs = super().forward(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "#         logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "#         probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs_base = self.ft_model.forward(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "#             logits_base = outputs_base.logits[:, -1, :]  # Get logits of the last token\n",
    "#             probs_base = torch.nn.functional.softmax(logits_base, dim=-1)\n",
    "\n",
    "#         # print('middle', torch.max(probs_base, dim=-1))\n",
    "#         probs = probs*probs_base\n",
    "#         sum_probs = probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "#         # Avoid division by zero by adding a small value (epsilon)\n",
    "#         sum_probs = torch.clamp(sum_probs, min=1e-9)\n",
    "\n",
    "#         # Re-normalize by dividing each probability by the sum of probabilities\n",
    "#         probs = probs / sum_probs\n",
    "\n",
    "#         next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "\n",
    "#         generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "#         # Extend the attention mask to include the newly generated token\n",
    "#         new_attention_mask = torch.ones((attention_mask.shape[0], 1)).to(input_ids.device)\n",
    "#         attention_mask = torch.cat((attention_mask, new_attention_mask), dim=-1)\n",
    "        \n",
    "#         modified_logits = torch.log(probs + 1e-8)\n",
    "        modified_logits = torch.empty(input_ids.size(0), TARGET_SIZE, 50257).to(input_ids.device)  # Empty tensor of the desired final size\n",
    "        with torch.no_grad():\n",
    "            position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "        # past_key_values = None\n",
    "        # past_key_values_base = None\n",
    "\n",
    "        \n",
    "        # print('modified_logits', modified_logits.size())\n",
    " \n",
    "        for step in range(TARGET_SIZE):\n",
    "            # Get the model outputs (logits) for the current step\n",
    "            # with torch.no_grad():\n",
    "            #     position_ids = get_position_ids(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "            # print('generated_ids', generated_ids.size())\n",
    "            # print('attention_mask', attention_mask.size())\n",
    "            # print('position_ids', position_ids.size())\n",
    "            # if(past_key_values):\n",
    "            #     outputs = super().forward(input_ids=generated_ids[:, -1:], attention_mask=attention_mask[:, -1:], \n",
    "            #                               position_ids=position_ids[:, -1:], use_cache=True, past_key_values=past_key_values)\n",
    "            # else:\n",
    "            outputs = super().forward(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "            logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # past_key_values = outputs.past_key_values\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # if(past_key_values_base):\n",
    "                #     outputs_base = self.ft_model.forward(input_ids=generated_ids[:, -1:], attention_mask=attention_mask[:, -1:], position_ids=position_ids[:, -1:], \n",
    "                #                                          use_cache=True, past_key_values=past_key_values_base)\n",
    "                # else:\n",
    "                outputs_base = self.ft_model.forward(input_ids=generated_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "                logits_base = outputs_base.logits[:, -1, :]  # Get logits of the last token\n",
    "                probs_base = torch.nn.functional.softmax(logits_base, dim=-1)\n",
    "                # past_key_values_base = outputs_base.past_key_values\n",
    "                    \n",
    "            # print('middle', torch.max(probs_base, dim=-1))\n",
    "            probs = probs*probs_base\n",
    "            sum_probs = probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # Avoid division by zero by adding a small value (epsilon)\n",
    "            sum_probs = torch.clamp(sum_probs, min=1e-9)\n",
    "\n",
    "            # Re-normalize by dividing each probability by the sum of probabilities\n",
    "            probs = probs / sum_probs\n",
    "            \n",
    "            next_token = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            generated_ids = torch.cat((generated_ids, next_token), dim=-1)\n",
    "            # Extend the attention mask to include the newly generated token\n",
    "            new_attention_mask = torch.ones((attention_mask.shape[0], 1)).to(input_ids.device)\n",
    "            attention_mask = torch.cat((attention_mask, new_attention_mask), dim=-1)\n",
    "            \n",
    "            temp_logits = torch.log(probs + 1e-8) \n",
    "            \n",
    "            # modified_logits = torch.cat((modified_logits, temp_logits), dim=1)\n",
    "            modified_logits[:, step, :] = temp_logits\n",
    "            \n",
    "            last_values = position_ids[:, -1]  # This gets the last value of each row (shape: m)\n",
    "            new_values = last_values + 1  # Increment each last value by 1\n",
    "            new_values = new_values.unsqueeze(1)  # Reshape to (m, 1) to concatenate with the tensor\n",
    "            position_ids = torch.cat([position_ids, new_values], dim=1) \n",
    "            \n",
    "        # print('modified_logits', modified_logits.size())\n",
    "\n",
    "            \n",
    "#         # Get the logits from the base GPT-2 model\n",
    "#         position_ids = get_position_ids(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         outputs = super().forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "#         logits = outputs.logits\n",
    "#         probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "#             logits_base = outputs_base.logits\n",
    "#             probabilities_base = nn.functional.softmax(logits_base, dim=-1)\n",
    "\n",
    "#         weighted_probabilities = probabilities * probabilities_base\n",
    "#         # weighted_probabilities = probabilities\n",
    "        \n",
    "#         # Normalize the probabilities again to ensure they sum to 1\n",
    "#         normalized_probabilities = weighted_probabilities / weighted_probabilities.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "#         # Convert back to logits (unnormalized scores) by applying log after multiplying probabilities\n",
    "#         modified_logits = torch.log(normalized_probabilities + 1e-8)  # Add small constant to avoid log(0)\n",
    "        \n",
    "        # Use modified logits for loss calculation or generation\n",
    "        if labels is not None:\n",
    "#             shift_logits = modified_logits[..., :-1, :].contiguous()\n",
    "#             shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            ### IF BATCH THEN MOVE IT, OTHERWISE NOT\n",
    "            \n",
    "            shift_logits = modified_logits\n",
    "            shift_labels = labels\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, modified_logits\n",
    "        \n",
    "        return modified_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_probs(md, input_ids, attention_mask, position_ids):\n",
    "    with torch.no_grad():\n",
    "        position_ids_base = position_ids.to(md.device)\n",
    "        input_ids_base = input_ids.to(md.device)\n",
    "        attention_mask_base = attention_mask.to(md.device)\n",
    "        # print('reached  here as well')\n",
    "        # print('input_ids_base device', input_ids_base.device)\n",
    "        # print('self.ft_model device', model_ft.device)\n",
    "        outputs_base = md.forward(input_ids_base, attention_mask=attention_mask_base, position_ids=position_ids_base)\n",
    "        # outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask)\n",
    "        # print(type(outputs_base))\n",
    "        logits_base = outputs_base.logits\n",
    "        # Convert logits to probabilities (apply softmax)\n",
    "        probabilities_base = nn.functional.softmax(logits_base, dim=-1)\n",
    "    return probabilities_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLlamaModelBatchSeparate(LlamaForCausalLM):\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        super().resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get the logits from the base GPT-2 model\n",
    "        position_ids = get_position_ids(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # print('got position ids')\n",
    "        # print('$$$', input_ids.device)\n",
    "        # print(attention_mask)\n",
    "        # print(position_ids)\n",
    "        # print(labels)\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        # outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        # print('logits', logits.size())\n",
    "        # Convert logits to probabilities (apply softmax)\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        # print('probabilities', probabilities.size())\n",
    "        \n",
    "        # print('no error here')\n",
    "        probabilities_base = get_base_probs(input_ids, attention_mask, position_ids)\n",
    "        \n",
    "        probabilities_base = probabilities_base.to(probabilities.device)\n",
    "        # print('no error here as well')\n",
    "        \n",
    "        # Multiply the probabilities by the custom vector\n",
    "        # Ensure that the custom vector shape matches the logits shape [batch_size, seq_len, vocab_size]\n",
    "        # For this example, assume the vector applies element-wise across the vocabulary dimension\n",
    "        weighted_probabilities = probabilities * probabilities_base\n",
    "        # weighted_probabilities = probabilities\n",
    "        \n",
    "        # Normalize the probabilities again to ensure they sum to 1\n",
    "        normalized_probabilities = weighted_probabilities / weighted_probabilities.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Convert back to logits (unnormalized scores) by applying log after multiplying probabilities\n",
    "        modified_logits = torch.log(normalized_probabilities + 1e-8)  # Add small constant to avoid log(0)\n",
    "        \n",
    "        # Use modified logits for loss calculation or generation\n",
    "        if labels is not None:\n",
    "            # print('reached here')\n",
    "            # print('labels size', labels.size())\n",
    "            # print('labels', labels)\n",
    "            # Shift the logits and labels for computing the loss\n",
    "            shift_logits = modified_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # print('shift_logits', shift_logits.size())\n",
    "            # print('shift_labels', shift_labels.size())            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, modified_logits\n",
    "        \n",
    "        return modified_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlamaModelBatch(LlamaForCausalLM):\n",
    "    def __init__(self, config, ft_model, tokenizer):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.ft_model = ft_model\n",
    "        super().resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        torch.nn.DataParallel(super(), device_ids=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "        self.ft_model.to('cpu')\n",
    "\n",
    "        # for param in self.base_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    # def to(self, device):\n",
    "    #     super().to(device)  # Move only small_model to GPU\n",
    "    #     self.ft_model.to('cpu')\n",
    "    #     return self\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get the logits from the base GPT-2 model\n",
    "        position_ids = get_position_ids(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print('got position ids')\n",
    "        print('$$$', input_ids.device)\n",
    "        # print(attention_mask)\n",
    "        # print(position_ids)\n",
    "        # print(labels)\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        # outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        # print('logits', logits.size())\n",
    "        # Convert logits to probabilities (apply softmax)\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        print('probabilities', probabilities.size())\n",
    "        \n",
    "        # print('no error here')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            position_ids_base = position_ids.to('cpu')\n",
    "            input_ids_base = input_ids.to('cpu')\n",
    "            attention_mask_base = attention_mask.to('cpu')\n",
    "            print('reached  here as well')\n",
    "            print('input_ids_base device', input_ids_base.device)\n",
    "            print('self.ft_model device', self.ft_model.device)\n",
    "            outputs_base = self.ft_model.forward(input_ids_base, attention_mask=attention_mask_base, position_ids=position_ids_base)\n",
    "            # outputs_base = self.ft_model.forward(input_ids, attention_mask=attention_mask)\n",
    "            # print(type(outputs_base))\n",
    "            logits_base = outputs_base.logits\n",
    "            # Convert logits to probabilities (apply softmax)\n",
    "            probabilities_base = nn.functional.softmax(logits_base, dim=-1)\n",
    "        \n",
    "        probabilities_base = probabilities_base.to(probabilities.device)\n",
    "        print('no error here as well')\n",
    "        \n",
    "        # Multiply the probabilities by the custom vector\n",
    "        # Ensure that the custom vector shape matches the logits shape [batch_size, seq_len, vocab_size]\n",
    "        # For this example, assume the vector applies element-wise across the vocabulary dimension\n",
    "        weighted_probabilities = probabilities * probabilities_base\n",
    "        # weighted_probabilities = probabilities\n",
    "        \n",
    "        # Normalize the probabilities again to ensure they sum to 1\n",
    "        normalized_probabilities = weighted_probabilities / weighted_probabilities.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Convert back to logits (unnormalized scores) by applying log after multiplying probabilities\n",
    "        modified_logits = torch.log(normalized_probabilities + 1e-8)  # Add small constant to avoid log(0)\n",
    "        \n",
    "        # Use modified logits for loss calculation or generation\n",
    "        if labels is not None:\n",
    "            # print('reached here')\n",
    "            # print('labels size', labels.size())\n",
    "            # print('labels', labels)\n",
    "            # Shift the logits and labels for computing the loss\n",
    "            shift_logits = modified_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # print('shift_logits', shift_logits.size())\n",
    "            # print('shift_labels', shift_labels.size())            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, modified_logits\n",
    "        \n",
    "        return modified_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define a smaller configuration\n",
    "# # config = GPT2Config(\n",
    "# #     n_layer=1,         # Fewer transformer layers\n",
    "# #     n_embd=64,        # Smaller hidden size (default is 768)\n",
    "# #     n_head=1,          # Fewer attention heads (default is 12)\n",
    "# #     resid_pdrop=0.2,   # Increase dropout to reduce overfitting\n",
    "# #     attn_pdrop=0.2,    # Increase dropout in the attention layer\n",
    "# #     embd_pdrop=0.2,    # Increase dropout in the embedding layer\n",
    "# #     initializer_range=0.02,  # Smaller initialization range to stabilize training\n",
    "# # )\n",
    "if(model_type == 'gpt2'):\n",
    "    config = GPT2Config(\n",
    "        n_layer=1,         # Fewer transformer layers (default 12)\n",
    "    )\n",
    "elif(model_type == 'llama'):\n",
    "    config = LlamaConfig(\n",
    "        num_hidden_layers=1,         # Fewer transformer layers (default 12)\n",
    "        vocab_size=len(tokenizer.vocab),\n",
    "        hidden_size=512,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=2048,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(model_type=='gpt2'):\n",
    "    model = CustomGPT2ModelBatch(config, model_ft)\n",
    "# model = GPT2SmallBatch.from_pretrained('gpt2', model_ft)\n",
    "else:\n",
    "    # model = CustomLlamaModelBatch(config, model_ft, tokenizer)\n",
    "    model = CustomLlamaModelBatchSeparate(config, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if(param.requires_grad):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"../../results/plugin\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=5e-6,\n",
    "#     per_device_train_batch_size=8,  # This batch size is per GPU\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=10,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='../../logs/plugin',\n",
    "#     logging_steps=1,\n",
    "#     push_to_hub=False\n",
    "# )\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{PROC_NAME}',\n",
    "    evaluation_strategy='epoch',\n",
    "    # eval_steps=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=8,  # This batch size is per GPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=1,\n",
    "    logging_dir=f'./logs/{PROC_NAME}',\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,  # Enable mixed-precision training for faster training\n",
    "    report_to=\"none\",\n",
    "    # load_best_model_at_end=True,  # Required for early stopping\n",
    "    # metric_for_best_model=\"eval_loss\",  # Metric to determine the best model (optional)\n",
    "    # greater_is_better=False,  # Set to False if lower metric is better (e.g., loss)\n",
    "    save_total_limit=1,\n",
    "    # no_cuda = True,\n",
    "    # dataloader_num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accelerator for multi-GPU support\n",
    "# train_dataset = tokenized_dataset['train']\n",
    "# eval_dataset = tokenized_dataset['hypervalidation']\n",
    "\n",
    "train_dataset = tokenized_dataset['validation']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# model, train_dataset, eval_dataset = accelerator.prepare(model, train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collate function to convert lists to tensors on-the-fly\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask'], dtype=torch.long) for item in batch]\n",
    "    labels = [torch.tensor(item['labels'], dtype=torch.long) for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_mask),\n",
    "        'labels': torch.stack(labels),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "total_steps = len(train_dataset) * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),  # Warm-up for the first 10% of steps\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "# # Set the model to training mode\n",
    "# model.train()\n",
    "\n",
    "# # Tokenize some example data\n",
    "# # inputs = tokenizer(\"Example input text\", return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "# # labels = tokenizer(\"Example output text\", return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "\n",
    "# # Initialize optimizer\n",
    "# # optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# # Basic training loop\n",
    "# for epoch in range(1):\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         input_ids = batch['input_ids'].to(model.device)\n",
    "#         attention_mask = batch['attention_mask'].to(model.device)\n",
    "#         labels = batch['labels'].to(model.device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss, logits = outputs\n",
    "#         total_loss+=loss.item()\n",
    "#         # print(f\"Batch {epoch} | Loss: {loss.item()}\")\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#     print(f'loss after epoch {epoch} is: {total_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                       # The model with PEFT applied\n",
    "    args=training_args,                     # Training arguments\n",
    "    train_dataset=train_dataset, # Training data\n",
    "    eval_dataset=eval_dataset, # Validation data\n",
    "    # data_collator=data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    optimizers=(\n",
    "        optimizer, \n",
    "        scheduler\n",
    "        ),  # Pass optimizer and scheduler\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]  # Add early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "# for name, param in check.named_parameters():\n",
    "#     if(param.requires_grad):\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "# # Compare the weights of the lm_head and the embedding layer\n",
    "# print(\"Are lm_head and wte sharing the same weights?\")\n",
    "# print(model.lm_head.weight is model.transformer.wte.weight)\n",
    "\n",
    "# # Compare the weights of the lm_head and the embedding layer\n",
    "# print(\"Are lm_head and wte sharing the same weights?\")\n",
    "# print(model.ft_model.lm_head.weight is model.ft_model.embed_tokens.weight)\n",
    "\n",
    "# # Compare the weights of the lm_head and the embedding layer\n",
    "# print(\"Are lm_head and wte sharing the same weights?\")\n",
    "# print(base_model.lm_head.weight is base_model.transformer.wte.weight)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)\n",
    "    # if(param.requires_grad):\n",
    "    #     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare the weights of the lm_head and the embedding layer\n",
    "# print(\"Are lm_head and wte sharing the same weights?\")\n",
    "# print(model.ft_model.lm_head.weight is model_ft.transformer.wte.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_ft.transformer.wte.weight, model_ft.transformer.h[1].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.ft_model.transformer.wte.weight, model.ft_model.transformer.h[1].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import DataParallel\n",
    "# model = DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 05:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.446700</td>\n",
       "      <td>2.510737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.357700</td>\n",
       "      <td>2.441068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ubuntu/statml/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=154, training_loss=2.4397337096078053, metrics={'train_runtime': 359.8245, 'train_samples_per_second': 23.895, 'train_steps_per_second': 0.428, 'total_flos': 691984877617152.0, 'train_loss': 2.4397337096078053, 'epoch': 2.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accelerator.wait_for_everyone()  # Synchronize GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "transformer.wte.weight True\n",
      "ft_model.transformer.wte.weight False\n"
     ]
    }
   ],
   "source": [
    "# for name, param in check.named_parameters():\n",
    "#     if(param.requires_grad):\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(model.lm_head.weight is model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(model.ft_model.lm_head.weight is model.ft_model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(base_model.lm_head.weight is base_model.transformer.wte.weight)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)\n",
    "    # if(param.requires_grad):\n",
    "    #     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/plugin_over_base_gpt2_small_full_ft_ft_model_no_token_without_lora_train_hyperval_concatenated/tokenizer_config.json',\n",
       " './models/plugin_over_base_gpt2_small_full_ft_ft_model_no_token_without_lora_train_hyperval_concatenated/special_tokens_map.json',\n",
       " './models/plugin_over_base_gpt2_small_full_ft_ft_model_no_token_without_lora_train_hyperval_concatenated/vocab.json',\n",
       " './models/plugin_over_base_gpt2_small_full_ft_ft_model_no_token_without_lora_train_hyperval_concatenated/merges.txt',\n",
       " './models/plugin_over_base_gpt2_small_full_ft_ft_model_no_token_without_lora_train_hyperval_concatenated/added_tokens.json',\n",
       " './models/plugin_over_base_gpt2_small_full_ft_ft_model_no_token_without_lora_train_hyperval_concatenated/tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(f'./models/{PROC_NAME}_{FT_MODEL_NAME}')\n",
    "tokenizer.save_pretrained(f'./models/{PROC_NAME}_{FT_MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, param in check.named_parameters():\n",
    "#     if(param.requires_grad):\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(model.lm_head.weight is model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(model.ft_model.lm_head.weight is model.ft_model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(base_model.lm_head.weight is base_model.transformer.wte.weight)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)\n",
    "    # if(param.requires_grad):\n",
    "    #     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = AutoModelForCausalLM.from_pretrained(f'./models/test{PROC_NAME}_{FT_MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "transformer.wte.weight True\n",
      "ft_model.transformer.wte.weight False\n"
     ]
    }
   ],
   "source": [
    "# for name, param in check.named_parameters():\n",
    "#     if(param.requires_grad):\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(model.lm_head.weight is model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(model.ft_model.lm_head.weight is model.ft_model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(base_model.lm_head.weight is base_model.transformer.wte.weight)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)\n",
    "    # if(param.requires_grad):\n",
    "    #     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are lm_head and wte sharing the same weights?\n",
      "True\n",
      "Are lm_head and wte sharing the same weights?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'ft_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compare the weights of the lm_head and the embedding layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAre lm_head and wte sharing the same weights?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mft_model\u001b[49m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;129;01mis\u001b[39;00m check\u001b[38;5;241m.\u001b[39mft_model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compare the weights of the lm_head and the embedding layer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAre lm_head and wte sharing the same weights?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'ft_model'"
     ]
    }
   ],
   "source": [
    "# for name, param in check.named_parameters():\n",
    "#     if(param.requires_grad):\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(check.lm_head.weight is check.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(check.ft_model.lm_head.weight is check.ft_model.transformer.wte.weight)\n",
    "\n",
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(base_model.lm_head.weight is base_model.transformer.wte.weight)\n",
    "\n",
    "for name, param in check.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)\n",
    "    # if(param.requires_grad):\n",
    "    #     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are lm_head and wte sharing the same weights?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare the weights of the lm_head and the embedding layer\n",
    "print(\"Are lm_head and wte sharing the same weights?\")\n",
    "print(base_model.lm_head.weight is base_model.transformer.wte.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight True\n",
      "ft_model.transformer.wte.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if('wte' in name or 'lm_head' in name):\n",
    "        print(name, param.requires_grad)\n",
    "    # if(param.requires_grad):\n",
    "    #     print(name, param.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
